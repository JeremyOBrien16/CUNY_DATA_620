{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620 - Project 3\n",
    "\n",
    "Jeremy OBrien, Mael Illien, Vanita Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. \n",
    "* Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. \n",
    "* Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. \n",
    "* Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "* How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "* Source: Natural Language Processing with Python, exercise 6.10.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We will be working with the 'names' corpus contained within NLTK.\n",
    "\n",
    "We will be starting our work by restructing some of the content from Chapter 6. The function 'test_classifier' will encapsulate most of the work by splitting the data into train and test sets, extracting features, training the model and predicting on the test set. \n",
    "\n",
    "A section will be dedicated to feature engineering by buiding on the example features from the book. The three classifiers from Chapter 6 were NaiveBayes, DecisionTree, MaxEntropy. Each classifier will be studied in its respective section. A summary table will be provided in order to easily compare the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import nltk, re, pprint\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of models will be constructed in this document which will be saved in a list as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [] # Will contain tuples (model, model_name, gender_features, classifier_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK 'names' corpus contains both male and female names in different text files. The code below extracts the names from both files, assigns the gender, and stores the information in a list of tuples. The labeled names are shuffled to avoid getting getting a single gender in a train or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Christie', 'female'),\n",
       " ('Tibold', 'male'),\n",
       " ('Chet', 'male'),\n",
       " ('Alyss', 'female'),\n",
       " ('Eunice', 'female'),\n",
       " ('Mehetabel', 'female'),\n",
       " ('Marj', 'female'),\n",
       " ('Adam', 'male'),\n",
       " ('Natka', 'female'),\n",
       " ('Sarene', 'female')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.seed(620)\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation) MI: This section is not necessary. It is built into the test test_classifier function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function (JO: uncommented for use in testing, can recomment out for submission)\n",
    "train_names = labeled_names[:500]\n",
    "devtest_names = labeled_names[500:1000]\n",
    "test_names = labeled_names[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function (JO: uncommented for use in testing, can recomment out for submission)\n",
    "# train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "# devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "# test_set = [(gender_features(n), gender) for (n, gender) in test_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Classifier\n",
    "\n",
    "The test_classifier function takes in a corpus of names, as well as a function to extract features and a classifier type. It splits the datasets into 3: train, devtest and test. The classifier model is then trained and the its accuracy on both test sets printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(names_corpus, gender_features_function, classifier_type):\n",
    "#     train_set = apply_features(gender_features, names[:500])\n",
    "#     devtest_set = apply_features(gender_features, names[500:1000])\n",
    "#     test_set = apply_features(gender_features, names[1000:])\n",
    "\n",
    "    # Train test split\n",
    "    train_names = names_corpus[:500]\n",
    "    devtest_names = names_corpus[500:1000]\n",
    "    test_names = names_corpus[1000:]\n",
    "    \n",
    "    # Appy features\n",
    "    train_set = [(gender_features_function(n), gender) for (n, gender) in train_names]\n",
    "    devtest_set = [(gender_features_function(n), gender) for (n, gender) in devtest_names]\n",
    "    test_set = [(gender_features_function(n), gender) for (n, gender) in test_names]\n",
    "    \n",
    "    # Classify and print score\n",
    "    classifier = classifier_type.train(train_set)\n",
    "    acc_devtest = nltk.classify.accuracy(classifier, devtest_set)\n",
    "    acc_test = nltk.classify.accuracy(classifier, test_set)\n",
    "    print(acc_devtest)\n",
    "    print(acc_test)\n",
    "    \n",
    "    #classifier.show_most_informative_features(5)\n",
    "    \n",
    "    class_name = classifier_type.__name__\n",
    "    gf_name = gender_features_function.__name__\n",
    "    models.append((classifier, class_name, gf_name, acc_devtest, acc_test))\n",
    "    \n",
    "    return classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(classifier):\n",
    "    errors = []\n",
    "    for (name, tag) in devtest_names:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, name) )\n",
    "            \n",
    "    for (tag, guess, name) in sorted(errors): \n",
    "        print('correct=%-8s guess=%-8s name=%-30s'%(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender_features2('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(word):\n",
    "    return {'suffix1': word[-1:],'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix1': 'a', 'suffix2': 'na'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('Cristina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    features['suffix1'] =  name[-1:]\n",
    "    features['suffix2'] = name[-2:]\n",
    "    features['suffix3'] = name[-3:]\n",
    "    # features['length'] = len(name) # doesn't add much\n",
    "    #suf = []\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "\n",
    "In this section, we define the model parameters and call the test_classifier function. Accuracy scores on both test sets are printed for each model.\n",
    "\n",
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.758\n",
      "0.756336405529954\n"
     ]
    }
   ],
   "source": [
    "nb_mod1 = test_classifier(labeled_names, gender_features, nltk.NaiveBayesClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     45.9 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.7 : 1.0\n",
      "             last_letter = 'o'              male : female =      8.9 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.5 : 1.0\n",
      "             last_letter = 'i'            female : male   =      4.7 : 1.0\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "nb_mod1.show_most_informative_features(5)\n",
    "print(nb_mod1.classify(gender_features('Neo')))\n",
    "print(nb_mod1.classify(gender_features('Trinity')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.738\n",
      "0.753168202764977\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     45.9 : 1.0\n",
      "              lastletter = 'd'              male : female =      9.7 : 1.0\n",
      "              lastletter = 'o'              male : female =      8.9 : 1.0\n",
      "              lastletter = 'r'              male : female =      6.5 : 1.0\n",
      "              lastletter = 'i'            female : male   =      4.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_mod2 = test_classifier(labeled_names, gender_features2, nltk.NaiveBayesClassifier)\n",
    "nb_mod2.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n",
      "0.7656970046082949\n",
      "Most Informative Features\n",
      "                 suffix1 = 'a'            female : male   =     45.9 : 1.0\n",
      "                 suffix2 = 'ne'           female : male   =     10.2 : 1.0\n",
      "                 suffix1 = 'd'              male : female =      9.7 : 1.0\n",
      "                 suffix1 = 'o'              male : female =      8.9 : 1.0\n",
      "                 suffix1 = 'r'              male : female =      6.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_mod3 = test_classifier(labeled_names, gender_features3, nltk.NaiveBayesClassifier)\n",
    "nb_mod3.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78\n",
      "0.7786578341013825\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     45.9 : 1.0\n",
      "                 suffix1 = 'a'            female : male   =     45.9 : 1.0\n",
      "                 suffix2 = 'ne'           female : male   =     10.2 : 1.0\n",
      "              lastletter = 'd'              male : female =      9.7 : 1.0\n",
      "                 suffix1 = 'd'              male : female =      9.7 : 1.0\n",
      "              lastletter = 'o'              male : female =      8.9 : 1.0\n",
      "                 suffix1 = 'o'              male : female =      8.9 : 1.0\n",
      "                 suffix3 = 'ine'          female : male   =      6.5 : 1.0\n",
      "              lastletter = 'r'              male : female =      6.5 : 1.0\n",
      "                 suffix1 = 'r'              male : female =      6.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_mod4 = test_classifier(labeled_names, gender_features4, nltk.NaiveBayesClassifier)\n",
    "nb_mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756\n",
      "0.7622407834101382\n"
     ]
    }
   ],
   "source": [
    "dt_mod1 = test_classifier(labeled_names, gender_features, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.7226382488479263\n"
     ]
    }
   ],
   "source": [
    "dt_mod2 = test_classifier(labeled_names, gender_features2, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n",
      "0.732286866359447\n"
     ]
    }
   ],
   "source": [
    "dt_mod3 = test_classifier(labeled_names, gender_features3, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "0.637528801843318\n"
     ]
    }
   ],
   "source": [
    "dt_mod4 = test_classifier(labeled_names, gender_features4, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using probabilites to set model parameters as the Naive Bayes classifier does, the Maximum Entropy Model (or MaxEnt) searches for the set of parameters that maximize model performance. The property of entropy entails uniformity of the distribution where there isn't empirical evidence that would constrain that uniformity.  \n",
    "\n",
    "Intuition is that classifiers with lower entropy introduce biases that are not justified. \n",
    "\n",
    "Importantly, MaxEnt does not assume independence of features (as Naive Bayes does) and so is not negatively impacted when there is dependence between features (can often be the case). As MaxEnt captures the structure of the training data, the more features it uses the stronger the constraint of empirical consistenycy becomes (reference)[https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/classifying/nochunks.html#maxent].\n",
    "\n",
    "For each joint feature (define!), MaxEnt algorithms calculate the empirical frequency and...(complete!).\n",
    "\n",
    "NLTK offers two algorithms, Generalized Iterative Scaling (GIS) and Improved Iterative Scaling (IIS), which offers faster convergence. Accoring to wikipedia (link!) and the literature ('A comparison of algorithms for maximum entropy parameter estimation')[http://luthuli.cs.uiuc.edu/~daf/courses/Opt-2017/Papers/p18-malouf.pdf], the performance of these algorithms has been substantially improved uppon by gradient-based methods, such as coordinate descent and limited memory L-BFGS and LMBVM. Most notably, iterative optimizations can be time consuming\n",
    "\n",
    "Additionally, MaxEnt is a conditional classifier, meaning it can be used to determine the most likely label for a given input or conversely how likely a label is for that input.  A generative classifier like Naive Bayers can estimate the most likely input value, how likely an input value is, the same given an input label. \n",
    "\n",
    "https://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers.pdf (cite!) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid comuptation, peaks after single iteration\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.37629        0.758\n",
      "             3          -0.37386        0.758\n",
      "             4          -0.37241        0.758\n",
      "             5          -0.37144        0.758\n",
      "             6          -0.37075        0.758\n",
      "             7          -0.37024        0.758\n",
      "             8          -0.36983        0.758\n",
      "             9          -0.36951        0.758\n",
      "            10          -0.36925        0.758\n",
      "            11          -0.36903        0.758\n",
      "            12          -0.36884        0.758\n",
      "            13          -0.36869        0.758\n",
      "            14          -0.36855        0.758\n",
      "            15          -0.36843        0.758\n",
      "            16          -0.36832        0.758\n",
      "            17          -0.36823        0.758\n",
      "            18          -0.36814        0.758\n",
      "            19          -0.36807        0.758\n",
      "            20          -0.36800        0.758\n",
      "            21          -0.36793        0.758\n",
      "            22          -0.36788        0.758\n",
      "            23          -0.36782        0.758\n",
      "            24          -0.36778        0.758\n",
      "            25          -0.36773        0.758\n",
      "            26          -0.36769        0.758\n",
      "            27          -0.36765        0.758\n",
      "            28          -0.36762        0.758\n",
      "            29          -0.36758        0.758\n",
      "            30          -0.36755        0.758\n",
      "            31          -0.36752        0.758\n",
      "            32          -0.36750        0.758\n",
      "            33          -0.36747        0.758\n",
      "            34          -0.36745        0.758\n",
      "            35          -0.36742        0.758\n",
      "            36          -0.36740        0.758\n",
      "            37          -0.36738        0.758\n",
      "            38          -0.36736        0.758\n",
      "            39          -0.36734        0.758\n",
      "            40          -0.36733        0.758\n",
      "            41          -0.36731        0.758\n",
      "            42          -0.36729        0.758\n",
      "            43          -0.36728        0.758\n",
      "            44          -0.36726        0.758\n",
      "            45          -0.36725        0.758\n",
      "            46          -0.36724        0.758\n",
      "            47          -0.36722        0.758\n",
      "            48          -0.36721        0.758\n",
      "            49          -0.36720        0.758\n",
      "            50          -0.36719        0.758\n",
      "            51          -0.36718        0.758\n",
      "            52          -0.36717        0.758\n",
      "            53          -0.36716        0.758\n",
      "            54          -0.36715        0.758\n",
      "            55          -0.36714        0.758\n",
      "            56          -0.36713        0.758\n",
      "            57          -0.36712        0.758\n",
      "            58          -0.36711        0.758\n",
      "            59          -0.36710        0.758\n",
      "            60          -0.36709        0.758\n",
      "            61          -0.36709        0.758\n",
      "            62          -0.36708        0.758\n",
      "            63          -0.36707        0.758\n",
      "            64          -0.36707        0.758\n",
      "            65          -0.36706        0.758\n",
      "            66          -0.36705        0.758\n",
      "            67          -0.36705        0.758\n",
      "            68          -0.36704        0.758\n",
      "            69          -0.36703        0.758\n",
      "            70          -0.36703        0.758\n",
      "            71          -0.36702        0.758\n",
      "            72          -0.36702        0.758\n",
      "            73          -0.36701        0.758\n",
      "            74          -0.36701        0.758\n",
      "            75          -0.36700        0.758\n",
      "            76          -0.36700        0.758\n",
      "            77          -0.36699        0.758\n",
      "            78          -0.36699        0.758\n",
      "            79          -0.36698        0.758\n",
      "            80          -0.36698        0.758\n",
      "            81          -0.36697        0.758\n",
      "            82          -0.36697        0.758\n",
      "            83          -0.36697        0.758\n",
      "            84          -0.36696        0.758\n",
      "            85          -0.36696        0.758\n",
      "            86          -0.36695        0.758\n",
      "            87          -0.36695        0.758\n",
      "            88          -0.36695        0.758\n",
      "            89          -0.36694        0.758\n",
      "            90          -0.36694        0.758\n",
      "            91          -0.36694        0.758\n",
      "            92          -0.36693        0.758\n",
      "            93          -0.36693        0.758\n",
      "            94          -0.36693        0.758\n",
      "            95          -0.36692        0.758\n",
      "            96          -0.36692        0.758\n",
      "            97          -0.36692        0.758\n",
      "            98          -0.36691        0.758\n",
      "            99          -0.36691        0.758\n",
      "         Final          -0.36691        0.758\n",
      "0.756\n",
      "0.762528801843318\n",
      "   6.644 last_letter=='k' and label is 'male'\n",
      "   6.644 last_letter=='x' and label is 'male'\n",
      "   6.644 last_letter=='b' and label is 'male'\n",
      "   6.644 last_letter=='z' and label is 'female'\n",
      "   6.644 last_letter=='p' and label is 'male'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "  -5.858 last_letter=='a' and label is 'male'\n",
      "  -2.392 last_letter=='i' and label is 'male'\n",
      "  -2.000 last_letter=='d' and label is 'female'\n",
      "  -1.807 last_letter=='o' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_mod1 = test_classifier(labeled_names, gender_features, nltk.ConditionalExponentialClassifier)  # consider changing max_iter param to 20, how to add kwarg?\n",
    "# https://stackoverflow.com/questions/39391280/how-to-change-number-of-iterations-in-maxent-classifier-for-pos-tagging-in-nltk\n",
    "\n",
    "me_mod1.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#me_mod1.explain(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(me_mod1.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.60663        0.632\n",
      "             3          -0.59048        0.632\n",
      "             4          -0.57533        0.632\n",
      "             5          -0.56114        0.638\n",
      "             6          -0.54787        0.664\n",
      "             7          -0.53547        0.684\n",
      "             8          -0.52387        0.694\n",
      "             9          -0.51302        0.716\n",
      "            10          -0.50286        0.728\n",
      "            11          -0.49334        0.742\n",
      "            12          -0.48442        0.756\n",
      "            13          -0.47604        0.750\n",
      "            14          -0.46816        0.760\n",
      "            15          -0.46074        0.762\n",
      "            16          -0.45374        0.768\n",
      "            17          -0.44714        0.764\n",
      "            18          -0.44090        0.764\n",
      "            19          -0.43500        0.768\n",
      "            20          -0.42941        0.780\n",
      "            21          -0.42411        0.784\n",
      "            22          -0.41907        0.786\n",
      "            23          -0.41429        0.792\n",
      "            24          -0.40973        0.804\n",
      "            25          -0.40539        0.802\n",
      "            26          -0.40126        0.806\n",
      "            27          -0.39731        0.808\n",
      "            28          -0.39353        0.812\n",
      "            29          -0.38993        0.814\n",
      "            30          -0.38647        0.816\n",
      "            31          -0.38316        0.816\n",
      "            32          -0.37999        0.824\n",
      "            33          -0.37694        0.830\n",
      "            34          -0.37402        0.830\n",
      "            35          -0.37121        0.830\n",
      "            36          -0.36851        0.832\n",
      "            37          -0.36591        0.834\n",
      "            38          -0.36340        0.836\n",
      "            39          -0.36099        0.836\n",
      "            40          -0.35866        0.836\n",
      "            41          -0.35641        0.836\n",
      "            42          -0.35424        0.836\n",
      "            43          -0.35215        0.834\n",
      "            44          -0.35012        0.834\n",
      "            45          -0.34816        0.834\n",
      "            46          -0.34626        0.836\n",
      "            47          -0.34443        0.836\n",
      "            48          -0.34265        0.836\n",
      "            49          -0.34092        0.836\n",
      "            50          -0.33925        0.834\n",
      "            51          -0.33763        0.834\n",
      "            52          -0.33605        0.840\n",
      "            53          -0.33452        0.840\n",
      "            54          -0.33303        0.842\n",
      "            55          -0.33159        0.844\n",
      "            56          -0.33018        0.844\n",
      "            57          -0.32881        0.844\n",
      "            58          -0.32748        0.846\n",
      "            59          -0.32619        0.846\n",
      "            60          -0.32492        0.846\n",
      "            61          -0.32369        0.846\n",
      "            62          -0.32249        0.848\n",
      "            63          -0.32132        0.846\n",
      "            64          -0.32018        0.846\n",
      "            65          -0.31907        0.844\n",
      "            66          -0.31798        0.844\n",
      "            67          -0.31692        0.844\n",
      "            68          -0.31589        0.842\n",
      "            69          -0.31487        0.842\n",
      "            70          -0.31388        0.842\n",
      "            71          -0.31292        0.842\n",
      "            72          -0.31197        0.842\n",
      "            73          -0.31105        0.842\n",
      "            74          -0.31014        0.842\n",
      "            75          -0.30925        0.842\n",
      "            76          -0.30838        0.842\n",
      "            77          -0.30753        0.842\n",
      "            78          -0.30670        0.842\n",
      "            79          -0.30589        0.840\n",
      "            80          -0.30509        0.840\n",
      "            81          -0.30430        0.840\n",
      "            82          -0.30353        0.836\n",
      "            83          -0.30278        0.836\n",
      "            84          -0.30204        0.836\n",
      "            85          -0.30131        0.836\n",
      "            86          -0.30060        0.836\n",
      "            87          -0.29990        0.840\n",
      "            88          -0.29922        0.842\n",
      "            89          -0.29854        0.846\n",
      "            90          -0.29788        0.846\n",
      "            91          -0.29723        0.846\n",
      "            92          -0.29659        0.848\n",
      "            93          -0.29596        0.848\n",
      "            94          -0.29534        0.848\n",
      "            95          -0.29473        0.848\n",
      "            96          -0.29414        0.848\n",
      "            97          -0.29355        0.848\n",
      "            98          -0.29297        0.848\n",
      "            99          -0.29240        0.848\n",
      "         Final          -0.29184        0.848\n",
      "0.766\n",
      "0.7821140552995391\n",
      "  -5.130 lastletter=='a' and label is 'male'\n",
      "   3.347 firstletter=='u' and label is 'female'\n",
      "   2.292 firstletter=='y' and label is 'male'\n",
      "  -2.175 lastletter=='i' and label is 'male'\n",
      "  -1.604 lastletter=='o' and label is 'female'\n",
      "  -1.590 lastletter=='d' and label is 'female'\n",
      "   1.563 lastletter=='c' and label is 'male'\n",
      "   1.501 count(e)==4 and label is 'female'\n",
      "   1.449 count(h)==3 and label is 'female'\n",
      "  -1.374 lastletter=='r' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_mod2 = test_classifier(labeled_names, gender_features2, nltk.ConditionalExponentialClassifier)\n",
    "me_mod2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow computation, peaks after six iterations\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.34422        0.816\n",
      "             3          -0.32017        0.830\n",
      "             4          -0.30386        0.834\n",
      "             5          -0.29234        0.836\n",
      "             6          -0.28389        0.838\n",
      "             7          -0.27743        0.840\n",
      "             8          -0.27234        0.840\n",
      "             9          -0.26820        0.840\n",
      "            10          -0.26477        0.840\n",
      "            11          -0.26187        0.840\n",
      "            12          -0.25940        0.840\n",
      "            13          -0.25725        0.840\n",
      "            14          -0.25537        0.840\n",
      "            15          -0.25372        0.840\n",
      "            16          -0.25225        0.840\n",
      "            17          -0.25093        0.840\n",
      "            18          -0.24975        0.840\n",
      "            19          -0.24868        0.840\n",
      "            20          -0.24771        0.840\n",
      "            21          -0.24682        0.840\n",
      "            22          -0.24601        0.840\n",
      "            23          -0.24526        0.840\n",
      "            24          -0.24457        0.840\n",
      "            25          -0.24393        0.840\n",
      "            26          -0.24333        0.840\n",
      "            27          -0.24278        0.840\n",
      "            28          -0.24227        0.840\n",
      "            29          -0.24179        0.840\n",
      "            30          -0.24133        0.840\n",
      "            31          -0.24091        0.840\n",
      "            32          -0.24051        0.840\n",
      "            33          -0.24014        0.840\n",
      "            34          -0.23978        0.840\n",
      "            35          -0.23945        0.840\n",
      "            36          -0.23913        0.840\n",
      "            37          -0.23883        0.840\n",
      "            38          -0.23854        0.840\n",
      "            39          -0.23827        0.840\n",
      "            40          -0.23801        0.840\n",
      "            41          -0.23776        0.840\n",
      "            42          -0.23753        0.840\n",
      "            43          -0.23730        0.840\n",
      "            44          -0.23709        0.840\n",
      "            45          -0.23688        0.840\n",
      "            46          -0.23668        0.840\n",
      "            47          -0.23650        0.840\n",
      "            48          -0.23631        0.840\n",
      "            49          -0.23614        0.840\n",
      "            50          -0.23597        0.840\n",
      "            51          -0.23581        0.840\n",
      "            52          -0.23566        0.840\n",
      "            53          -0.23551        0.840\n",
      "            54          -0.23536        0.840\n",
      "            55          -0.23522        0.840\n",
      "            56          -0.23509        0.840\n",
      "            57          -0.23496        0.840\n",
      "            58          -0.23483        0.840\n",
      "            59          -0.23471        0.840\n",
      "            60          -0.23460        0.840\n",
      "            61          -0.23448        0.840\n",
      "            62          -0.23437        0.840\n",
      "            63          -0.23427        0.840\n",
      "            64          -0.23416        0.840\n",
      "            65          -0.23406        0.840\n",
      "            66          -0.23396        0.840\n",
      "            67          -0.23387        0.840\n",
      "            68          -0.23378        0.840\n",
      "            69          -0.23369        0.840\n",
      "            70          -0.23360        0.840\n",
      "            71          -0.23352        0.840\n",
      "            72          -0.23344        0.840\n",
      "            73          -0.23336        0.840\n",
      "            74          -0.23328        0.840\n",
      "            75          -0.23320        0.840\n",
      "            76          -0.23313        0.840\n",
      "            77          -0.23306        0.840\n",
      "            78          -0.23299        0.840\n",
      "            79          -0.23292        0.840\n",
      "            80          -0.23285        0.840\n",
      "            81          -0.23279        0.840\n",
      "            82          -0.23273        0.840\n",
      "            83          -0.23266        0.840\n",
      "            84          -0.23260        0.840\n",
      "            85          -0.23254        0.840\n",
      "            86          -0.23249        0.840\n",
      "            87          -0.23243        0.840\n",
      "            88          -0.23238        0.840\n",
      "            89          -0.23232        0.840\n",
      "            90          -0.23227        0.840\n",
      "            91          -0.23222        0.840\n",
      "            92          -0.23217        0.840\n",
      "            93          -0.23212        0.840\n",
      "            94          -0.23207        0.840\n",
      "            95          -0.23202        0.840\n",
      "            96          -0.23198        0.840\n",
      "            97          -0.23193        0.840\n",
      "            98          -0.23189        0.840\n",
      "            99          -0.23184        0.840\n",
      "         Final          -0.23180        0.840\n",
      "0.758\n",
      "0.7626728110599078\n",
      "  15.029 suffix2=='ha' and label is 'male'\n",
      "  11.066 suffix2=='vi' and label is 'male'\n",
      "  10.253 suffix2=='ko' and label is 'female'\n",
      "  -9.849 suffix1=='a' and label is 'male'\n",
      "   8.238 suffix2=='Em' and label is 'female'\n",
      "   7.264 suffix2=='ev' and label is 'female'\n",
      "   6.848 suffix2=='es' and label is 'female'\n",
      "   6.848 suffix2=='ss' and label is 'female'\n",
      "   6.439 suffix2=='me' and label is 'male'\n",
      "   6.439 suffix2=='fe' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "me_mod3 = test_classifier(labeled_names, gender_features3, nltk.ConditionalExponentialClassifier)\n",
    "me_mod3.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid computation, unclear if reached optimum as continues to improve\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.58951        0.632\n",
      "             3          -0.55937        0.632\n",
      "             4          -0.53237        0.656\n",
      "             5          -0.50821        0.714\n",
      "             6          -0.48661        0.744\n",
      "             7          -0.46726        0.782\n",
      "             8          -0.44989        0.800\n",
      "             9          -0.43425        0.820\n",
      "            10          -0.42012        0.838\n",
      "            11          -0.40730        0.858\n",
      "            12          -0.39564        0.868\n",
      "            13          -0.38498        0.872\n",
      "            14          -0.37520        0.878\n",
      "            15          -0.36620        0.880\n",
      "            16          -0.35788        0.878\n",
      "            17          -0.35017        0.882\n",
      "            18          -0.34300        0.882\n",
      "            19          -0.33631        0.882\n",
      "            20          -0.33005        0.888\n",
      "            21          -0.32418        0.892\n",
      "            22          -0.31865        0.894\n",
      "            23          -0.31344        0.900\n",
      "            24          -0.30851        0.902\n",
      "            25          -0.30384        0.904\n",
      "            26          -0.29940        0.906\n",
      "            27          -0.29518        0.912\n",
      "            28          -0.29116        0.912\n",
      "            29          -0.28731        0.916\n",
      "            30          -0.28363        0.916\n",
      "            31          -0.28011        0.918\n",
      "            32          -0.27673        0.920\n",
      "            33          -0.27348        0.922\n",
      "            34          -0.27035        0.922\n",
      "            35          -0.26733        0.922\n",
      "            36          -0.26442        0.924\n",
      "            37          -0.26161        0.926\n",
      "            38          -0.25890        0.924\n",
      "            39          -0.25627        0.928\n",
      "            40          -0.25372        0.928\n",
      "            41          -0.25125        0.928\n",
      "            42          -0.24886        0.928\n",
      "            43          -0.24653        0.930\n",
      "            44          -0.24427        0.930\n",
      "            45          -0.24207        0.930\n",
      "            46          -0.23993        0.934\n",
      "            47          -0.23784        0.934\n",
      "            48          -0.23581        0.934\n",
      "            49          -0.23383        0.936\n",
      "            50          -0.23190        0.936\n",
      "            51          -0.23001        0.940\n",
      "            52          -0.22817        0.940\n",
      "            53          -0.22637        0.940\n",
      "            54          -0.22460        0.942\n",
      "            55          -0.22288        0.946\n",
      "            56          -0.22120        0.950\n",
      "            57          -0.21955        0.950\n",
      "            58          -0.21794        0.950\n",
      "            59          -0.21635        0.950\n",
      "            60          -0.21481        0.952\n",
      "            61          -0.21329        0.952\n",
      "            62          -0.21180        0.952\n",
      "            63          -0.21034        0.954\n",
      "            64          -0.20891        0.954\n"
     ]
    }
   ],
   "source": [
    "me_mod4 = test_classifier(labeled_names, gender_features4, nltk.ConditionalExponentialClassifier)\n",
    "me_mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_models(models):\n",
    "    table = pd.DataFrame(columns = ['class', 'features', 'accuracy_devtest', 'accuracy_test'])\n",
    "    \n",
    "    for m in models:\n",
    "        df = pd.DataFrame({'class': [m[1]], 'features': [m[2]], 'accuracy_devtest': [m[3]], 'accuracy_test': [m[4]]})\n",
    "        table = table.append(df, ignore_index=True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = summarize_models(models)\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.sort_values(by='accuracy_test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
