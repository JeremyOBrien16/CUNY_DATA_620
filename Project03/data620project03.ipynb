{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620 - Project 3\n",
    "\n",
    "Jeremy OBrien, Mael Illien, Vanita Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. \n",
    "* Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. \n",
    "* Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. \n",
    "* Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "* How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "* Source: Natural Language Processing with Python, exercise 6.10.2.\n",
    "\n",
    "The three classifiers from Chapter 6: NaiveBayes, DecisionTree, MaxEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk, re, pprint\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Petunia', 'female'),\n",
       " ('Angelita', 'female'),\n",
       " ('Margit', 'female'),\n",
       " ('Bathsheba', 'female'),\n",
       " ('Garland', 'female'),\n",
       " ('Flemming', 'male'),\n",
       " ('Monty', 'male'),\n",
       " ('Karylin', 'female'),\n",
       " ('Chelton', 'male'),\n",
       " ('Pepillo', 'male')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function\n",
    "# train_names = labeled_names[:500]\n",
    "# devtest_names = labeled_names[500:1000]\n",
    "# test_names = labeled_names[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function\n",
    "# train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "# devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "# test_set = [(gender_features(n), gender) for (n, gender) in test_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(names_corpus, gender_features_function, classifier_type):\n",
    "#     train_set = apply_features(gender_features, names[:500])\n",
    "#     devtest_set = apply_features(gender_features, names[500:1000])\n",
    "#     test_set = apply_features(gender_features, names[1000:])\n",
    "\n",
    "    # Train test split\n",
    "    train_names = names_corpus[:500]\n",
    "    devtest_names = names_corpus[500:1000]\n",
    "    test_names = names_corpus[1000:]\n",
    "    \n",
    "    # Appy features\n",
    "    train_set = [(gender_features_function(n), gender) for (n, gender) in train_names]\n",
    "    devtest_set = [(gender_features_function(n), gender) for (n, gender) in devtest_names]\n",
    "    test_set = [(gender_features_function(n), gender) for (n, gender) in test_names]\n",
    "    \n",
    "    # Classify and print score\n",
    "    classifier = classifier_type.train(train_set)\n",
    "    print(nltk.classify.accuracy(classifier, devtest_set))\n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    \n",
    "    #classifier.show_most_informative_features(5)\n",
    "    \n",
    "    return classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(classifier):\n",
    "    errors = []\n",
    "    for (name, tag) in devtest_names:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, name) )\n",
    "            \n",
    "    for (tag, guess, name) in sorted(errors): \n",
    "        print('correct=%-8s guess=%-8s name=%-30s'%(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender_features2('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(word):\n",
    "    return {'suffix1': word[-1:],'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix1': 'a', 'suffix2': 'na'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('Cristina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    features['suffix1'] =  name[-1:]\n",
    "    features['suffix2'] = name[-2:]\n",
    "    features['suffix3'] = name[-3:]\n",
    "    # features['length'] = len(name) # doesn't add much\n",
    "    #suf = []\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.762\n",
      "0.7452476958525346\n"
     ]
    }
   ],
   "source": [
    "mod1 = test_classifier(labeled_names, gender_features, nltk.NaiveBayesClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     16.4 : 1.0\n",
      "             last_letter = 's'              male : female =     15.5 : 1.0\n",
      "             last_letter = 'o'              male : female =     15.5 : 1.0\n",
      "             last_letter = 'r'              male : female =      6.3 : 1.0\n",
      "             last_letter = 'k'              male : female =      5.5 : 1.0\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "mod1.show_most_informative_features(5)\n",
    "print(mod1.classify(gender_features('Neo')))\n",
    "print(mod1.classify(gender_features('Trinity')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.722\n",
      "0.7386232718894009\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     16.4 : 1.0\n",
      "              lastletter = 'o'              male : female =     15.5 : 1.0\n",
      "              lastletter = 's'              male : female =     15.5 : 1.0\n",
      "             firstletter = 'h'              male : female =      6.9 : 1.0\n",
      "              lastletter = 'r'              male : female =      6.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod2 = test_classifier(labeled_names, gender_features2, nltk.NaiveBayesClassifier)\n",
    "mod2.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774\n",
      "0.7564804147465438\n",
      "Most Informative Features\n",
      "                 suffix1 = 'a'            female : male   =     16.4 : 1.0\n",
      "                 suffix1 = 'o'              male : female =     15.5 : 1.0\n",
      "                 suffix1 = 's'              male : female =     15.5 : 1.0\n",
      "                 suffix1 = 'r'              male : female =      6.3 : 1.0\n",
      "                 suffix1 = 'k'              male : female =      5.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod3 = test_classifier(labeled_names, gender_features3, nltk.NaiveBayesClassifier)\n",
    "mod3.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n",
      "0.7785138248847926\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     16.4 : 1.0\n",
      "                 suffix1 = 'a'            female : male   =     16.4 : 1.0\n",
      "                 suffix1 = 'o'              male : female =     15.5 : 1.0\n",
      "              lastletter = 'o'              male : female =     15.5 : 1.0\n",
      "              lastletter = 's'              male : female =     15.5 : 1.0\n",
      "                 suffix1 = 's'              male : female =     15.5 : 1.0\n",
      "             firstletter = 'h'              male : female =      6.9 : 1.0\n",
      "              lastletter = 'r'              male : female =      6.3 : 1.0\n",
      "                 suffix1 = 'r'              male : female =      6.3 : 1.0\n",
      "             firstletter = 'w'              male : female =      6.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod4 = test_classifier(labeled_names, gender_features4, nltk.NaiveBayesClassifier)\n",
    "mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.686\n",
      "0.663594470046083\n"
     ]
    }
   ],
   "source": [
    "dt_mod1 = test_classifier(labeled_names, gender_features4, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per the text:\n",
    "Instead of using probabilites to set model params, search to find set of params that max model performance throug iterative optimization techniques (which can be time consuming).\n",
    "\n",
    "Generalization of the Naive Bayes classifier model.\n",
    "\n",
    "For each joint feature, Max Ent calculcate the empirical frequency of that features.\n",
    "\n",
    "Conditional classifier: can be used to determine what is most likely label for given input or how likely fiven label is for given input. P(label|input).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notes: https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/classifying/nochunks.html#maxent\n",
    "\n",
    "Maximum Entropy Classifier is not negatively impacted by feature interdependence as Naive Bayes (which assumes independence) is.  \n",
    "Use classifiers that are empirically consistent with training data, meaning estimate of frequency of each feature is equal to actual.  \n",
    "This captures the structure of the training data.  More features uses, the stronger the constraint of empirical consistency becomes.\n",
    "\n",
    "Intuition is that 'classifiers with lower entropy introduce biases that are not justified'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid comuptation, peaks after single iteration\n",
    "\n",
    "ADD CHART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.396\n",
      "             2          -0.40885        0.742\n",
      "             3          -0.40384        0.742\n",
      "             4          -0.40085        0.742\n",
      "             5          -0.39887        0.742\n",
      "             6          -0.39745        0.742\n",
      "             7          -0.39638        0.742\n",
      "             8          -0.39556        0.742\n",
      "             9          -0.39490        0.742\n",
      "            10          -0.39436        0.742\n",
      "            11          -0.39391        0.742\n",
      "            12          -0.39353        0.742\n",
      "            13          -0.39320        0.742\n",
      "            14          -0.39292        0.742\n",
      "            15          -0.39268        0.742\n",
      "            16          -0.39246        0.742\n",
      "            17          -0.39226        0.742\n",
      "            18          -0.39209        0.742\n",
      "            19          -0.39194        0.742\n",
      "            20          -0.39179        0.742\n",
      "            21          -0.39167        0.742\n",
      "            22          -0.39155        0.742\n",
      "            23          -0.39144        0.742\n",
      "            24          -0.39134        0.742\n",
      "            25          -0.39125        0.742\n",
      "            26          -0.39117        0.742\n",
      "            27          -0.39109        0.742\n",
      "            28          -0.39102        0.742\n",
      "            29          -0.39095        0.742\n",
      "            30          -0.39089        0.742\n",
      "            31          -0.39083        0.742\n",
      "            32          -0.39077        0.742\n",
      "            33          -0.39072        0.742\n",
      "            34          -0.39067        0.742\n",
      "            35          -0.39062        0.742\n",
      "            36          -0.39058        0.742\n",
      "            37          -0.39053        0.742\n",
      "            38          -0.39049        0.742\n",
      "            39          -0.39046        0.742\n",
      "            40          -0.39042        0.742\n",
      "            41          -0.39039        0.742\n",
      "            42          -0.39035        0.742\n",
      "            43          -0.39032        0.742\n",
      "            44          -0.39029        0.742\n",
      "            45          -0.39026        0.742\n",
      "            46          -0.39024        0.742\n",
      "            47          -0.39021        0.742\n",
      "            48          -0.39019        0.742\n",
      "            49          -0.39016        0.742\n",
      "            50          -0.39014        0.742\n",
      "            51          -0.39012        0.742\n",
      "            52          -0.39009        0.742\n",
      "            53          -0.39007        0.742\n",
      "            54          -0.39005        0.742\n",
      "            55          -0.39003        0.742\n",
      "            56          -0.39002        0.742\n",
      "            57          -0.39000        0.742\n",
      "            58          -0.38998        0.742\n",
      "            59          -0.38996        0.742\n",
      "            60          -0.38995        0.742\n",
      "            61          -0.38993        0.742\n",
      "            62          -0.38992        0.742\n",
      "            63          -0.38990        0.742\n",
      "            64          -0.38989        0.742\n",
      "            65          -0.38987        0.742\n",
      "            66          -0.38986        0.742\n",
      "            67          -0.38985        0.742\n",
      "            68          -0.38984        0.742\n",
      "            69          -0.38982        0.742\n",
      "            70          -0.38981        0.742\n",
      "            71          -0.38980        0.742\n",
      "            72          -0.38979        0.742\n",
      "            73          -0.38978        0.742\n",
      "            74          -0.38977        0.742\n",
      "            75          -0.38976        0.742\n",
      "            76          -0.38975        0.742\n",
      "            77          -0.38974        0.742\n",
      "            78          -0.38973        0.742\n",
      "            79          -0.38972        0.742\n",
      "            80          -0.38971        0.742\n",
      "            81          -0.38970        0.742\n",
      "            82          -0.38969        0.742\n",
      "            83          -0.38968        0.742\n",
      "            84          -0.38967        0.742\n",
      "            85          -0.38967        0.742\n",
      "            86          -0.38966        0.742\n",
      "            87          -0.38965        0.742\n",
      "            88          -0.38964        0.742\n",
      "            89          -0.38964        0.742\n",
      "            90          -0.38963        0.742\n",
      "            91          -0.38962        0.742\n",
      "            92          -0.38962        0.742\n",
      "            93          -0.38961        0.742\n",
      "            94          -0.38960        0.742\n",
      "            95          -0.38960        0.742\n",
      "            96          -0.38959        0.742\n",
      "            97          -0.38958        0.742\n",
      "            98          -0.38958        0.742\n",
      "            99          -0.38957        0.742\n",
      "         Final          -0.38956        0.742\n",
      "0.76\n",
      "0.7485599078341014\n",
      "   6.644 last_letter=='m' and label is 'male'\n",
      "   6.644 last_letter=='w' and label is 'male'\n",
      "   6.644 last_letter=='p' and label is 'male'\n",
      "   6.644 last_letter=='b' and label is 'male'\n",
      "   6.644 last_letter=='x' and label is 'female'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "   6.644 last_letter=='u' and label is 'female'\n",
      "   6.644 last_letter=='f' and label is 'male'\n",
      "  -3.833 last_letter=='a' and label is 'male'\n",
      "  -3.000 last_letter=='o' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_mod1 = test_classifier(labeled_names, gender_features, nltk.ConditionalExponentialClassifier)\n",
    "me_mod1.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.396\n",
      "             2          -0.62973        0.604\n",
      "             3          -0.61106        0.604\n",
      "             4          -0.59370        0.628\n",
      "             5          -0.57761        0.658\n",
      "             6          -0.56269        0.692\n",
      "             7          -0.54888        0.716\n",
      "             8          -0.53608        0.736\n",
      "             9          -0.52421        0.754\n",
      "            10          -0.51319        0.768\n",
      "            11          -0.50294        0.770\n",
      "            12          -0.49340        0.770\n",
      "            13          -0.48451        0.774\n",
      "            14          -0.47620        0.780\n",
      "            15          -0.46842        0.784\n",
      "            16          -0.46112        0.794\n",
      "            17          -0.45427        0.794\n",
      "            18          -0.44783        0.794\n",
      "            19          -0.44176        0.798\n",
      "            20          -0.43603        0.800\n",
      "            21          -0.43062        0.800\n",
      "            22          -0.42550        0.802\n",
      "            23          -0.42064        0.808\n",
      "            24          -0.41603        0.812\n",
      "            25          -0.41165        0.810\n",
      "            26          -0.40748        0.814\n",
      "            27          -0.40351        0.810\n",
      "            28          -0.39972        0.812\n",
      "            29          -0.39611        0.812\n",
      "            30          -0.39265        0.814\n",
      "            31          -0.38934        0.814\n",
      "            32          -0.38617        0.818\n",
      "            33          -0.38314        0.820\n",
      "            34          -0.38023        0.820\n",
      "            35          -0.37743        0.822\n",
      "            36          -0.37474        0.818\n",
      "            37          -0.37215        0.816\n",
      "            38          -0.36966        0.816\n",
      "            39          -0.36727        0.818\n",
      "            40          -0.36496        0.818\n",
      "            41          -0.36273        0.816\n",
      "            42          -0.36057        0.818\n",
      "            43          -0.35850        0.816\n",
      "            44          -0.35649        0.816\n",
      "            45          -0.35454        0.818\n",
      "            46          -0.35266        0.822\n",
      "            47          -0.35084        0.822\n",
      "            48          -0.34908        0.822\n",
      "            49          -0.34737        0.824\n",
      "            50          -0.34572        0.824\n",
      "            51          -0.34411        0.824\n",
      "            52          -0.34255        0.824\n",
      "            53          -0.34103        0.824\n",
      "            54          -0.33956        0.824\n",
      "            55          -0.33813        0.826\n",
      "            56          -0.33674        0.826\n",
      "            57          -0.33539        0.824\n",
      "            58          -0.33407        0.824\n",
      "            59          -0.33279        0.824\n",
      "            60          -0.33154        0.824\n",
      "            61          -0.33032        0.824\n",
      "            62          -0.32914        0.826\n",
      "            63          -0.32798        0.828\n",
      "            64          -0.32685        0.828\n",
      "            65          -0.32575        0.828\n",
      "            66          -0.32468        0.828\n",
      "            67          -0.32363        0.826\n",
      "            68          -0.32261        0.826\n",
      "            69          -0.32160        0.828\n",
      "            70          -0.32063        0.828\n",
      "            71          -0.31967        0.830\n",
      "            72          -0.31874        0.828\n",
      "            73          -0.31782        0.830\n",
      "            74          -0.31693        0.830\n",
      "            75          -0.31605        0.830\n",
      "            76          -0.31520        0.830\n",
      "            77          -0.31436        0.830\n",
      "            78          -0.31353        0.830\n",
      "            79          -0.31273        0.830\n",
      "            80          -0.31194        0.830\n",
      "            81          -0.31117        0.830\n",
      "            82          -0.31041        0.832\n",
      "            83          -0.30967        0.830\n",
      "            84          -0.30894        0.830\n",
      "            85          -0.30822        0.830\n",
      "            86          -0.30752        0.830\n",
      "            87          -0.30683        0.830\n",
      "            88          -0.30616        0.830\n",
      "            89          -0.30549        0.830\n",
      "            90          -0.30484        0.830\n",
      "            91          -0.30420        0.830\n",
      "            92          -0.30357        0.830\n",
      "            93          -0.30295        0.830\n",
      "            94          -0.30235        0.830\n",
      "            95          -0.30175        0.830\n",
      "            96          -0.30116        0.830\n",
      "            97          -0.30058        0.830\n",
      "            98          -0.30002        0.832\n",
      "            99          -0.29946        0.834\n",
      "         Final          -0.29891        0.834\n",
      "0.75\n",
      "0.7711693548387096\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'me_mod2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-2e19ae823a5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mme_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgender_features2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConditionalExponentialClassifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mme_mod2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'me_mod2' is not defined"
     ]
    }
   ],
   "source": [
    "me_mod = test_classifier(labeled_names, gender_features2, nltk.ConditionalExponentialClassifier)\n",
    "me_mod2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow computation, peaks after six iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.396\n",
      "             2          -0.37462        0.792\n",
      "             3          -0.34529        0.818\n",
      "             4          -0.32621        0.822\n",
      "             5          -0.31325        0.824\n",
      "             6          -0.30403        0.826\n",
      "             7          -0.29714        0.828\n",
      "             8          -0.29181        0.828\n",
      "             9          -0.28753        0.828\n",
      "            10          -0.28402        0.828\n",
      "            11          -0.28108        0.828\n",
      "            12          -0.27858        0.828\n",
      "            13          -0.27643        0.828\n",
      "            14          -0.27455        0.828\n",
      "            15          -0.27290        0.828\n",
      "            16          -0.27144        0.828\n",
      "            17          -0.27013        0.828\n",
      "            18          -0.26896        0.828\n",
      "            19          -0.26790        0.828\n",
      "            20          -0.26693        0.828\n",
      "            21          -0.26606        0.828\n",
      "            22          -0.26525        0.828\n",
      "            23          -0.26452        0.828\n",
      "            24          -0.26384        0.828\n",
      "            25          -0.26321        0.828\n",
      "            26          -0.26263        0.828\n",
      "            27          -0.26209        0.828\n",
      "            28          -0.26158        0.828\n",
      "            29          -0.26111        0.828\n",
      "            30          -0.26067        0.828\n",
      "            31          -0.26025        0.828\n",
      "            32          -0.25986        0.828\n",
      "            33          -0.25949        0.828\n",
      "            34          -0.25915        0.828\n",
      "            35          -0.25882        0.828\n",
      "            36          -0.25851        0.828\n",
      "            37          -0.25821        0.828\n",
      "            38          -0.25793        0.828\n",
      "            39          -0.25767        0.828\n",
      "            40          -0.25742        0.828\n",
      "            41          -0.25718        0.828\n",
      "            42          -0.25695        0.828\n",
      "            43          -0.25673        0.828\n",
      "            44          -0.25652        0.828\n",
      "            45          -0.25632        0.828\n",
      "            46          -0.25612        0.828\n",
      "            47          -0.25594        0.828\n",
      "            48          -0.25576        0.828\n",
      "            49          -0.25559        0.828\n",
      "            50          -0.25543        0.828\n",
      "            51          -0.25527        0.828\n",
      "            52          -0.25512        0.828\n",
      "            53          -0.25498        0.828\n",
      "            54          -0.25484        0.828\n",
      "            55          -0.25470        0.828\n",
      "            56          -0.25457        0.828\n",
      "            57          -0.25445        0.828\n",
      "            58          -0.25432        0.828\n",
      "            59          -0.25421        0.828\n",
      "            60          -0.25409        0.828\n",
      "            61          -0.25398        0.828\n",
      "            62          -0.25387        0.828\n",
      "            63          -0.25377        0.828\n",
      "            64          -0.25367        0.828\n",
      "            65          -0.25357        0.828\n",
      "            66          -0.25348        0.828\n",
      "            67          -0.25339        0.828\n",
      "            68          -0.25330        0.828\n",
      "            69          -0.25321        0.828\n",
      "            70          -0.25313        0.828\n",
      "            71          -0.25305        0.828\n",
      "            72          -0.25297        0.828\n",
      "            73          -0.25289        0.828\n",
      "            74          -0.25281        0.828\n",
      "            75          -0.25274        0.828\n",
      "            76          -0.25267        0.828\n",
      "            77          -0.25260        0.828\n",
      "            78          -0.25253        0.828\n",
      "            79          -0.25247        0.828\n",
      "            80          -0.25240        0.828\n",
      "            81          -0.25234        0.828\n",
      "            82          -0.25228        0.828\n",
      "            83          -0.25222        0.828\n",
      "            84          -0.25216        0.828\n",
      "            85          -0.25210        0.828\n",
      "            86          -0.25205        0.828\n",
      "            87          -0.25199        0.828\n",
      "            88          -0.25194        0.828\n",
      "            89          -0.25189        0.828\n",
      "            90          -0.25183        0.828\n",
      "            91          -0.25178        0.828\n",
      "            92          -0.25174        0.828\n",
      "            93          -0.25169        0.828\n",
      "            94          -0.25164        0.828\n",
      "            95          -0.25160        0.828\n",
      "            96          -0.25155        0.828\n",
      "            97          -0.25151        0.828\n",
      "            98          -0.25146        0.828\n",
      "            99          -0.25142        0.828\n",
      "         Final          -0.25138        0.828\n",
      "0.76\n",
      "0.7537442396313364\n",
      "  13.146 suffix2=='ua' and label is 'male'\n",
      "  11.693 suffix2=='Bo' and label is 'female'\n",
      "   9.431 suffix2=='nk' and label is 'female'\n",
      "   8.238 suffix2=='rg' and label is 'female'\n",
      "  -7.859 suffix1=='a' and label is 'male'\n",
      "  -7.261 suffix1=='s' and label is 'female'\n",
      "   6.609 suffix2=='rt' and label is 'male'\n",
      "   6.510 suffix2=='be' and label is 'male'\n",
      "   6.510 suffix2=='me' and label is 'male'\n",
      "   6.510 suffix2=='oe' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "me_mod3 = test_classifier(labeled_names, gender_features3, nltk.ConditionalExponentialClassifier)\n",
    "me_mod3.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid computation, unclear if reached optimum as continues to improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.396\n",
      "             2          -0.61468        0.604\n",
      "             3          -0.58329        0.622\n",
      "             4          -0.55521        0.700\n",
      "             5          -0.53014        0.752\n",
      "             6          -0.50773        0.780\n",
      "             7          -0.48767        0.800\n",
      "             8          -0.46966        0.818\n",
      "             9          -0.45343        0.834\n",
      "            10          -0.43876        0.850\n",
      "            11          -0.42544        0.858\n",
      "            12          -0.41331        0.866\n",
      "            13          -0.40221        0.866\n",
      "            14          -0.39202        0.870\n",
      "            15          -0.38263        0.870\n",
      "            16          -0.37394        0.874\n",
      "            17          -0.36589        0.878\n",
      "            18          -0.35839        0.878\n",
      "            19          -0.35139        0.878\n",
      "            20          -0.34484        0.880\n",
      "            21          -0.33869        0.884\n",
      "            22          -0.33290        0.890\n",
      "            23          -0.32745        0.896\n",
      "            24          -0.32229        0.900\n",
      "            25          -0.31740        0.900\n",
      "            26          -0.31277        0.900\n",
      "            27          -0.30836        0.900\n",
      "            28          -0.30416        0.902\n",
      "            29          -0.30015        0.904\n",
      "            30          -0.29632        0.904\n",
      "            31          -0.29265        0.904\n",
      "            32          -0.28913        0.906\n",
      "            33          -0.28576        0.908\n",
      "            34          -0.28252        0.908\n",
      "            35          -0.27940        0.912\n",
      "            36          -0.27639        0.912\n",
      "            37          -0.27349        0.912\n",
      "            38          -0.27069        0.912\n",
      "            39          -0.26799        0.914\n",
      "            40          -0.26537        0.916\n",
      "            41          -0.26284        0.916\n",
      "            42          -0.26038        0.916\n",
      "            43          -0.25800        0.914\n",
      "            44          -0.25569        0.918\n",
      "            45          -0.25344        0.918\n",
      "            46          -0.25126        0.918\n",
      "            47          -0.24914        0.918\n",
      "            48          -0.24708        0.920\n",
      "            49          -0.24507        0.920\n",
      "            50          -0.24311        0.920\n",
      "            51          -0.24120        0.920\n",
      "            52          -0.23934        0.920\n",
      "            53          -0.23752        0.920\n",
      "            54          -0.23575        0.920\n",
      "            55          -0.23401        0.926\n",
      "            56          -0.23232        0.928\n",
      "            57          -0.23066        0.928\n",
      "            58          -0.22904        0.928\n",
      "            59          -0.22746        0.930\n",
      "            60          -0.22591        0.932\n",
      "            61          -0.22439        0.932\n",
      "            62          -0.22290        0.932\n",
      "            63          -0.22145        0.932\n",
      "            64          -0.22002        0.932\n",
      "            65          -0.21862        0.932\n",
      "            66          -0.21724        0.932\n",
      "            67          -0.21590        0.932\n",
      "            68          -0.21457        0.932\n",
      "            69          -0.21328        0.932\n",
      "            70          -0.21200        0.934\n",
      "            71          -0.21075        0.934\n",
      "            72          -0.20952        0.934\n",
      "            73          -0.20831        0.934\n",
      "            74          -0.20713        0.934\n",
      "            75          -0.20596        0.934\n",
      "            76          -0.20481        0.938\n",
      "            77          -0.20369        0.938\n",
      "            78          -0.20258        0.938\n",
      "            79          -0.20148        0.938\n",
      "            80          -0.20041        0.940\n",
      "            81          -0.19935        0.942\n",
      "            82          -0.19831        0.942\n",
      "            83          -0.19729        0.942\n",
      "            84          -0.19628        0.944\n",
      "            85          -0.19529        0.944\n",
      "            86          -0.19431        0.944\n",
      "            87          -0.19335        0.944\n",
      "            88          -0.19240        0.944\n",
      "            89          -0.19146        0.944\n",
      "            90          -0.19054        0.946\n",
      "            91          -0.18963        0.948\n",
      "            92          -0.18873        0.950\n",
      "            93          -0.18785        0.950\n",
      "            94          -0.18698        0.950\n",
      "            95          -0.18612        0.950\n",
      "            96          -0.18527        0.950\n",
      "            97          -0.18443        0.950\n",
      "            98          -0.18361        0.950\n",
      "            99          -0.18280        0.950\n",
      "         Final          -0.18199        0.950\n",
      "0.76\n",
      "0.7831221198156681\n",
      "   2.871 suffix3=='zra' and label is 'male'\n",
      "   2.808 suffix3=='wen' and label is 'female'\n",
      "   2.504 suffix2=='Bo' and label is 'female'\n",
      "   2.504 suffix3=='Bo' and label is 'female'\n",
      "   2.332 suffix3=='sha' and label is 'male'\n",
      "   2.052 suffix3=='ird' and label is 'female'\n",
      "   2.030 suffix3=='mey' and label is 'male'\n",
      "   2.007 suffix3=='arl' and label is 'male'\n",
      "  -1.924 lastletter=='a' and label is 'male'\n",
      "  -1.924 suffix1=='a' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "me_mod4 = test_classifier(labeled_names, gender_features4, nltk.ConditionalExponentialClassifier)\n",
    "me_mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
