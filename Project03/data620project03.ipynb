{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620 - Project 3\n",
    "\n",
    "Jeremy OBrien, Mael Illien, Vanita Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using any of the three classifiers described in chapter 6 of Natural Language Processing with Python, and any features you can think of, build the best name gender classifier you can. \n",
    "* Begin by splitting the Names Corpus into three subsets: 500 words for the test set, 500 words for the dev-test set, and the remaining 6900 words for the training set. \n",
    "* Then, starting with the example name gender classifier, make incremental improvements. Use the dev-test set to check your progress. \n",
    "* Once you are satisfied with your classifier, check its final performance on the test set. \n",
    "* How does the performance on the test set compare to the performance on the dev-test set? Is this what you'd expect?\n",
    "* Source: Natural Language Processing with Python, exercise 6.10.2.\n",
    "\n",
    "The three classifiers from Chapter 6: NaiveBayes, DecisionTree, MaxEntropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk, re, pprint\n",
    "from nltk.corpus import names\n",
    "from nltk.classify import apply_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Description of approach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Howard', 'male'),\n",
       " ('Kore', 'female'),\n",
       " ('Townie', 'male'),\n",
       " ('Jimbo', 'male'),\n",
       " ('Marney', 'female'),\n",
       " ('Alisun', 'female'),\n",
       " ('Lemuel', 'male'),\n",
       " ('Lilah', 'female'),\n",
       " ('Cris', 'female'),\n",
       " ('Sherrie', 'female')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_names = ([(name, 'male') for name in names.words('male.txt')] + \n",
    "         [(name, 'female') for name in names.words('female.txt')])\n",
    "\n",
    "random.shuffle(labeled_names)\n",
    "labeled_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function (JO: uncommented for use in testing, can recomment out for submission)\n",
    "train_names = labeled_names[:500]\n",
    "devtest_names = labeled_names[500:1000]\n",
    "test_names = labeled_names[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incorporated in function (JO: uncommented for use in testing, can recomment out for submission)\n",
    "train_set = [(gender_features(n), gender) for (n, gender) in train_names]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devtest_names]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in test_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(names_corpus, gender_features_function, classifier_type):\n",
    "#     train_set = apply_features(gender_features, names[:500])\n",
    "#     devtest_set = apply_features(gender_features, names[500:1000])\n",
    "#     test_set = apply_features(gender_features, names[1000:])\n",
    "\n",
    "    # Train test split\n",
    "    train_names = names_corpus[:500]\n",
    "    devtest_names = names_corpus[500:1000]\n",
    "    test_names = names_corpus[1000:]\n",
    "    \n",
    "    # Appy features\n",
    "    train_set = [(gender_features_function(n), gender) for (n, gender) in train_names]\n",
    "    devtest_set = [(gender_features_function(n), gender) for (n, gender) in devtest_names]\n",
    "    test_set = [(gender_features_function(n), gender) for (n, gender) in test_names]\n",
    "    \n",
    "    # Classify and print score\n",
    "    classifier = classifier_type.train(train_set)\n",
    "    print(nltk.classify.accuracy(classifier, devtest_set))\n",
    "    print(nltk.classify.accuracy(classifier, test_set))\n",
    "    \n",
    "    #classifier.show_most_informative_features(5)\n",
    "    \n",
    "    return classifier  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(classifier):\n",
    "    errors = []\n",
    "    for (name, tag) in devtest_names:\n",
    "        guess = classifier.classify(gender_features(name))\n",
    "        if guess != tag:\n",
    "            errors.append( (tag, guess, name) )\n",
    "            \n",
    "    for (tag, guess, name) in sorted(errors): \n",
    "        print('correct=%-8s guess=%-8s name=%-30s'%(tag, guess, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'last_letter': word[-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_letter': 'n'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features2(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gender_features2('John')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features3(word):\n",
    "    return {'suffix1': word[-1:],'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'suffix1': 'a', 'suffix2': 'na'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features3('Cristina')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features4(name):\n",
    "    features = {}\n",
    "    features[\"firstletter\"] = name[0].lower()\n",
    "    features[\"lastletter\"] = name[-1].lower()\n",
    "    features['suffix1'] =  name[-1:]\n",
    "    features['suffix2'] = name[-2:]\n",
    "    features['suffix3'] = name[-3:]\n",
    "    # features['length'] = len(name) # doesn't add much\n",
    "    #suf = []\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[\"count(%s)\" % letter] = name.lower().count(letter)\n",
    "        features[\"has(%s)\" % letter] = (letter in name.lower())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.774\n",
      "0.7442396313364056\n"
     ]
    }
   ],
   "source": [
    "mod1 = test_classifier(labeled_names, gender_features, nltk.NaiveBayesClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     27.2 : 1.0\n",
      "             last_letter = 'r'              male : female =     13.5 : 1.0\n",
      "             last_letter = 'd'              male : female =      9.4 : 1.0\n",
      "             last_letter = 't'              male : female =      8.1 : 1.0\n",
      "             last_letter = 'o'              male : female =      7.5 : 1.0\n",
      "male\n",
      "female\n"
     ]
    }
   ],
   "source": [
    "mod1.show_most_informative_features(5)\n",
    "print(mod1.classify(gender_features('Neo')))\n",
    "print(mod1.classify(gender_features('Trinity')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.772\n",
      "0.7482718894009217\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     27.2 : 1.0\n",
      "              lastletter = 'r'              male : female =     13.5 : 1.0\n",
      "              lastletter = 'd'              male : female =      9.4 : 1.0\n",
      "                count(o) = 2                male : female =      8.3 : 1.0\n",
      "              lastletter = 't'              male : female =      8.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod2 = test_classifier(labeled_names, gender_features2, nltk.NaiveBayesClassifier)\n",
    "mod2.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806\n",
      "0.7783698156682027\n",
      "Most Informative Features\n",
      "                 suffix1 = 'a'            female : male   =     27.2 : 1.0\n",
      "                 suffix1 = 'r'              male : female =     13.5 : 1.0\n",
      "                 suffix2 = 'er'             male : female =      9.4 : 1.0\n",
      "                 suffix1 = 'd'              male : female =      9.4 : 1.0\n",
      "                 suffix1 = 't'              male : female =      8.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod3 = test_classifier(labeled_names, gender_features3, nltk.NaiveBayesClassifier)\n",
    "mod3.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.796\n",
      "0.7874423963133641\n",
      "Most Informative Features\n",
      "              lastletter = 'a'            female : male   =     27.2 : 1.0\n",
      "                 suffix1 = 'a'            female : male   =     27.2 : 1.0\n",
      "                 suffix1 = 'r'              male : female =     13.5 : 1.0\n",
      "              lastletter = 'r'              male : female =     13.5 : 1.0\n",
      "                 suffix2 = 'er'             male : female =      9.4 : 1.0\n",
      "              lastletter = 'd'              male : female =      9.4 : 1.0\n",
      "                 suffix1 = 'd'              male : female =      9.4 : 1.0\n",
      "                count(o) = 2                male : female =      8.3 : 1.0\n",
      "                 suffix1 = 't'              male : female =      8.1 : 1.0\n",
      "              lastletter = 't'              male : female =      8.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "mod4 = test_classifier(labeled_names, gender_features4, nltk.NaiveBayesClassifier)\n",
    "mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.702\n",
      "0.6768433179723502\n"
     ]
    }
   ],
   "source": [
    "dt_mod1 = test_classifier(labeled_names, gender_features4, nltk.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy(labels):\n",
    "    freqdist = nltk.FreqDist(labels)\n",
    "    probs = [freqdist.freq(l) for l in freqdist]\n",
    "    return -sum(p * math.log(p,2) for p in probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using probabilites to set model parameters as the Naive Bayes classifier does, the Maximum Entropy Model (or MaxEnt) searches for the set of parameters that maximize model performance. The property of entropy entails uniformity of the distribution where there isn't empirical evidence that would constrain that uniformity.  \n",
    "\n",
    "Intuition is that classifiers with lower entropy introduce biases that are not justified. \n",
    "\n",
    "Importantly, MaxEnt does not assume independence of features (as Naive Bayes does) and so is not negatively impacted when there is dependence between features (can often be the case). As MaxEnt captures the structure of the training data, the more features it uses the stronger the constraint of empirical consistenycy becomes (reference)[https://lost-contact.mit.edu/afs/cs.pitt.edu/projects/nltk/docs/tutorial/classifying/nochunks.html#maxent].\n",
    "\n",
    "For each joint feature (define!), MaxEnt algorithms calculate the empirical frequency and...(complete!).\n",
    "\n",
    "NLTK offers two algorithms, Generalized Iterative Scaling (GIS) and Improved Iterative Scaling (IIS), which offers faster convergence. Accoring to wikipedia (link!) and the literature ('A comparison of algorithms for maximum entropy parameter estimation')[http://luthuli.cs.uiuc.edu/~daf/courses/Opt-2017/Papers/p18-malouf.pdf], the performance of these algorithms has been substantially improved uppon by gradient-based methods, such as coordinate descent and limited memory L-BFGS and LMBVM. Most notably, iterative optimizations can be time consuming\n",
    "\n",
    "Additionally, MaxEnt is a conditional classifier, meaning it can be used to determine the most likely label for a given input or conversely how likely a label is for that input.  A generative classifier like Naive Bayers can estimate the most likely input value, how likely an input value is, the same given an input label. \n",
    "\n",
    "https://web.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers.pdf (cite!) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid comuptation, peaks after single iteration\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.376\n",
      "             2          -0.36621        0.776\n",
      "             3          -0.36189        0.776\n",
      "             4          -0.35931        0.776\n",
      "             5          -0.35759        0.776\n",
      "             6          -0.35637        0.776\n",
      "             7          -0.35545        0.776\n",
      "             8          -0.35474        0.776\n",
      "             9          -0.35417        0.776\n",
      "            10          -0.35370        0.776\n",
      "            11          -0.35331        0.776\n",
      "            12          -0.35298        0.776\n",
      "            13          -0.35270        0.776\n",
      "            14          -0.35246        0.776\n",
      "            15          -0.35225        0.776\n",
      "            16          -0.35206        0.776\n",
      "            17          -0.35189        0.776\n",
      "            18          -0.35174        0.776\n",
      "            19          -0.35161        0.776\n",
      "            20          -0.35148        0.776\n",
      "            21          -0.35137        0.776\n",
      "            22          -0.35127        0.776\n",
      "            23          -0.35118        0.776\n",
      "            24          -0.35109        0.776\n",
      "            25          -0.35102        0.776\n",
      "            26          -0.35094        0.776\n",
      "            27          -0.35087        0.776\n",
      "            28          -0.35081        0.776\n",
      "            29          -0.35075        0.776\n",
      "            30          -0.35070        0.776\n",
      "            31          -0.35065        0.776\n",
      "            32          -0.35060        0.776\n",
      "            33          -0.35055        0.776\n",
      "            34          -0.35051        0.776\n",
      "            35          -0.35047        0.776\n",
      "            36          -0.35043        0.776\n",
      "            37          -0.35039        0.776\n",
      "            38          -0.35036        0.776\n",
      "            39          -0.35033        0.776\n",
      "            40          -0.35030        0.776\n",
      "            41          -0.35027        0.776\n",
      "            42          -0.35024        0.776\n",
      "            43          -0.35021        0.776\n",
      "            44          -0.35019        0.776\n",
      "            45          -0.35016        0.776\n",
      "            46          -0.35014        0.776\n",
      "            47          -0.35011        0.776\n",
      "            48          -0.35009        0.776\n",
      "            49          -0.35007        0.776\n",
      "            50          -0.35005        0.776\n",
      "            51          -0.35003        0.776\n",
      "            52          -0.35001        0.776\n",
      "            53          -0.35000        0.776\n",
      "            54          -0.34998        0.776\n",
      "            55          -0.34996        0.776\n",
      "            56          -0.34995        0.776\n",
      "            57          -0.34993        0.776\n",
      "            58          -0.34992        0.776\n",
      "            59          -0.34990        0.776\n",
      "            60          -0.34989        0.776\n",
      "            61          -0.34987        0.776\n",
      "            62          -0.34986        0.776\n",
      "            63          -0.34985        0.776\n",
      "            64          -0.34984        0.776\n",
      "            65          -0.34982        0.776\n",
      "            66          -0.34981        0.776\n",
      "            67          -0.34980        0.776\n",
      "            68          -0.34979        0.776\n",
      "            69          -0.34978        0.776\n",
      "            70          -0.34977        0.776\n",
      "            71          -0.34976        0.776\n",
      "            72          -0.34975        0.776\n",
      "            73          -0.34974        0.776\n",
      "            74          -0.34973        0.776\n",
      "            75          -0.34972        0.776\n",
      "            76          -0.34971        0.776\n",
      "            77          -0.34970        0.776\n",
      "            78          -0.34970        0.776\n",
      "            79          -0.34969        0.776\n",
      "            80          -0.34968        0.776\n",
      "            81          -0.34967        0.776\n",
      "            82          -0.34967        0.776\n",
      "            83          -0.34966        0.776\n",
      "            84          -0.34965        0.776\n",
      "            85          -0.34964        0.776\n",
      "            86          -0.34964        0.776\n",
      "            87          -0.34963        0.776\n",
      "            88          -0.34962        0.776\n",
      "            89          -0.34962        0.776\n",
      "            90          -0.34961        0.776\n",
      "            91          -0.34961        0.776\n",
      "            92          -0.34960        0.776\n",
      "            93          -0.34959        0.776\n",
      "            94          -0.34959        0.776\n",
      "            95          -0.34958        0.776\n",
      "            96          -0.34958        0.776\n",
      "            97          -0.34957        0.776\n",
      "            98          -0.34957        0.776\n",
      "            99          -0.34956        0.776\n",
      "         Final          -0.34956        0.776\n",
      "0.778\n",
      "0.7474078341013825\n",
      "   6.644 last_letter=='k' and label is 'male'\n",
      "   6.644 last_letter=='p' and label is 'male'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "   6.644 last_letter=='w' and label is 'male'\n",
      "   6.644 last_letter=='j' and label is 'male'\n",
      "   6.644 last_letter=='v' and label is 'male'\n",
      "   6.644 last_letter=='f' and label is 'male'\n",
      "  -4.807 last_letter=='a' and label is 'male'\n",
      "  -2.700 last_letter=='r' and label is 'female'\n",
      "  -2.000 last_letter=='d' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_mod1 = test_classifier(labeled_names, gender_features, nltk.ConditionalExponentialClassifier)  # consider changing max_iter param to 20, how to add kwarg?\n",
    "\n",
    "# https://stackoverflow.com/questions/39391280/how-to-change-number-of-iterations-in-maxent-classifier-for-pos-tagging-in-nltk\n",
    "\n",
    "me_mod1.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-d30d9c1c0c73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mme_mod1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mexplain\u001b[1;34m(self, featureset, columns)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mTEMPLATE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'  %-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescr_width\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m's%s%8.3f'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mpdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mprob_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logarithmic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, featureset, label)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;31m# Convert input-features to joint-features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m             \u001b[1;31m# Known feature name & value:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "me_mod1.explain(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6.644 last_letter=='k' and label is 'male'\n",
      "   6.644 last_letter=='p' and label is 'male'\n",
      "   6.644 last_letter=='c' and label is 'male'\n",
      "   6.644 last_letter=='w' and label is 'male'\n",
      "   6.644 last_letter=='j' and label is 'male'\n",
      "   6.644 last_letter=='v' and label is 'male'\n",
      "   6.644 last_letter=='f' and label is 'male'\n",
      "  -4.807 last_letter=='a' and label is 'male'\n",
      "  -2.700 last_letter=='r' and label is 'female'\n",
      "  -2.000 last_letter=='d' and label is 'female'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(me_mod1.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Explanation)\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.376\n",
      "             2          -0.61376        0.624\n",
      "             3          -0.59683        0.624\n",
      "             4          -0.58097        0.628\n",
      "             5          -0.56614        0.648\n",
      "             6          -0.55229        0.672\n",
      "             7          -0.53937        0.696\n",
      "             8          -0.52732        0.708\n",
      "             9          -0.51606        0.720\n",
      "            10          -0.50555        0.732\n",
      "            11          -0.49572        0.744\n",
      "            12          -0.48652        0.760\n",
      "            13          -0.47789        0.772\n",
      "            14          -0.46979        0.788\n",
      "            15          -0.46218        0.802\n",
      "            16          -0.45501        0.800\n",
      "            17          -0.44826        0.800\n",
      "            18          -0.44188        0.800\n",
      "            19          -0.43585        0.806\n",
      "            20          -0.43014        0.812\n",
      "            21          -0.42472        0.816\n",
      "            22          -0.41958        0.818\n",
      "            23          -0.41470        0.820\n",
      "            24          -0.41005        0.822\n",
      "            25          -0.40562        0.824\n",
      "            26          -0.40139        0.824\n",
      "            27          -0.39736        0.822\n",
      "            28          -0.39350        0.824\n",
      "            29          -0.38981        0.824\n",
      "            30          -0.38628        0.824\n",
      "            31          -0.38289        0.828\n",
      "            32          -0.37964        0.828\n",
      "            33          -0.37652        0.826\n",
      "            34          -0.37352        0.824\n",
      "            35          -0.37064        0.826\n",
      "            36          -0.36786        0.828\n",
      "            37          -0.36518        0.828\n",
      "            38          -0.36261        0.830\n",
      "            39          -0.36012        0.832\n",
      "            40          -0.35772        0.832\n",
      "            41          -0.35540        0.832\n",
      "            42          -0.35316        0.832\n",
      "            43          -0.35099        0.832\n",
      "            44          -0.34889        0.832\n",
      "            45          -0.34686        0.832\n",
      "            46          -0.34489        0.834\n",
      "            47          -0.34299        0.836\n",
      "            48          -0.34114        0.836\n",
      "            49          -0.33935        0.838\n",
      "            50          -0.33761        0.838\n",
      "            51          -0.33592        0.836\n",
      "            52          -0.33428        0.836\n",
      "            53          -0.33268        0.836\n",
      "            54          -0.33113        0.836\n",
      "            55          -0.32962        0.836\n",
      "            56          -0.32815        0.836\n",
      "            57          -0.32672        0.838\n",
      "            58          -0.32533        0.836\n",
      "            59          -0.32398        0.836\n",
      "            60          -0.32266        0.836\n",
      "            61          -0.32137        0.836\n",
      "            62          -0.32011        0.836\n",
      "            63          -0.31889        0.838\n",
      "            64          -0.31769        0.838\n",
      "            65          -0.31652        0.838\n",
      "            66          -0.31538        0.838\n",
      "            67          -0.31427        0.840\n",
      "            68          -0.31318        0.840\n",
      "            69          -0.31212        0.842\n",
      "            70          -0.31108        0.842\n",
      "            71          -0.31006        0.842\n",
      "            72          -0.30907        0.842\n",
      "            73          -0.30810        0.842\n",
      "            74          -0.30715        0.842\n",
      "            75          -0.30621        0.842\n",
      "            76          -0.30530        0.842\n",
      "            77          -0.30441        0.844\n",
      "            78          -0.30353        0.842\n",
      "            79          -0.30268        0.842\n",
      "            80          -0.30184        0.844\n",
      "            81          -0.30101        0.844\n",
      "            82          -0.30020        0.844\n",
      "            83          -0.29941        0.846\n",
      "            84          -0.29864        0.846\n",
      "            85          -0.29787        0.846\n",
      "            86          -0.29712        0.846\n",
      "            87          -0.29639        0.846\n",
      "            88          -0.29567        0.846\n",
      "            89          -0.29496        0.846\n",
      "            90          -0.29427        0.846\n",
      "            91          -0.29358        0.846\n",
      "            92          -0.29291        0.846\n",
      "            93          -0.29225        0.846\n",
      "            94          -0.29161        0.846\n",
      "            95          -0.29097        0.846\n",
      "            96          -0.29034        0.846\n",
      "            97          -0.28973        0.846\n",
      "            98          -0.28912        0.846\n",
      "            99          -0.28853        0.846\n",
      "         Final          -0.28794        0.846\n",
      "0.792\n",
      "0.7798099078341014\n",
      "  -3.871 lastletter=='a' and label is 'male'\n",
      "  -2.531 lastletter=='r' and label is 'female'\n",
      "  -1.784 lastletter=='t' and label is 'female'\n",
      "  -1.626 lastletter=='d' and label is 'female'\n",
      "   1.607 count(c)==2 and label is 'male'\n",
      "  -1.506 count(e)==3 and label is 'male'\n",
      "  -1.504 lastletter=='o' and label is 'female'\n",
      "   1.252 count(f)==2 and label is 'male'\n",
      "   1.224 lastletter=='j' and label is 'male'\n",
      "   1.199 count(z)==2 and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "me_mod2 = test_classifier(labeled_names, gender_features2, nltk.ConditionalExponentialClassifier)\n",
    "me_mod2.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow computation, peaks after six iterations\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.376\n",
      "             2          -0.33924        0.824\n",
      "             3          -0.31200        0.840\n",
      "             4          -0.29466        0.844\n",
      "             5          -0.28280        0.850\n",
      "             6          -0.27427        0.850\n",
      "             7          -0.26785        0.850\n",
      "             8          -0.26283        0.850\n",
      "             9          -0.25879        0.850\n",
      "            10          -0.25545        0.850\n",
      "            11          -0.25265        0.850\n",
      "            12          -0.25025        0.850\n",
      "            13          -0.24818        0.850\n",
      "            14          -0.24637        0.850\n",
      "            15          -0.24478        0.850\n",
      "            16          -0.24336        0.850\n",
      "            17          -0.24210        0.850\n",
      "            18          -0.24096        0.850\n",
      "            19          -0.23993        0.850\n",
      "            20          -0.23899        0.850\n",
      "            21          -0.23814        0.850\n",
      "            22          -0.23736        0.850\n",
      "            23          -0.23664        0.850\n",
      "            24          -0.23597        0.850\n",
      "            25          -0.23536        0.850\n",
      "            26          -0.23479        0.850\n",
      "            27          -0.23426        0.850\n",
      "            28          -0.23377        0.850\n",
      "            29          -0.23330        0.850\n",
      "            30          -0.23287        0.850\n",
      "            31          -0.23246        0.850\n",
      "            32          -0.23208        0.850\n",
      "            33          -0.23172        0.850\n",
      "            34          -0.23138        0.850\n",
      "            35          -0.23106        0.850\n",
      "            36          -0.23075        0.850\n",
      "            37          -0.23047        0.850\n",
      "            38          -0.23019        0.850\n",
      "            39          -0.22993        0.850\n",
      "            40          -0.22968        0.850\n",
      "            41          -0.22944        0.850\n",
      "            42          -0.22922        0.850\n",
      "            43          -0.22900        0.850\n",
      "            44          -0.22880        0.850\n",
      "            45          -0.22860        0.850\n",
      "            46          -0.22841        0.850\n",
      "            47          -0.22823        0.850\n",
      "            48          -0.22805        0.850\n",
      "            49          -0.22789        0.850\n",
      "            50          -0.22773        0.850\n",
      "            51          -0.22757        0.850\n",
      "            52          -0.22742        0.850\n",
      "            53          -0.22728        0.850\n",
      "            54          -0.22714        0.850\n",
      "            55          -0.22701        0.850\n",
      "            56          -0.22688        0.850\n",
      "            57          -0.22675        0.850\n",
      "            58          -0.22663        0.850\n",
      "            59          -0.22652        0.850\n",
      "            60          -0.22641        0.850\n",
      "            61          -0.22630        0.850\n",
      "            62          -0.22619        0.850\n",
      "            63          -0.22609        0.850\n",
      "            64          -0.22599        0.850\n",
      "            65          -0.22589        0.850\n",
      "            66          -0.22580        0.850\n",
      "            67          -0.22571        0.850\n",
      "            68          -0.22562        0.850\n",
      "            69          -0.22554        0.850\n",
      "            70          -0.22546        0.850\n",
      "            71          -0.22537        0.850\n",
      "            72          -0.22530        0.850\n",
      "            73          -0.22522        0.850\n",
      "            74          -0.22515        0.850\n",
      "            75          -0.22507        0.850\n",
      "            76          -0.22500        0.850\n",
      "            77          -0.22493        0.850\n",
      "            78          -0.22487        0.850\n",
      "            79          -0.22480        0.850\n",
      "            80          -0.22474        0.850\n",
      "            81          -0.22468        0.850\n",
      "            82          -0.22461        0.850\n",
      "            83          -0.22456        0.850\n",
      "            84          -0.22450        0.850\n",
      "            85          -0.22444        0.850\n",
      "            86          -0.22439        0.850\n",
      "            87          -0.22433        0.850\n",
      "            88          -0.22428        0.850\n",
      "            89          -0.22423        0.850\n",
      "            90          -0.22418        0.850\n",
      "            91          -0.22413        0.850\n",
      "            92          -0.22408        0.850\n",
      "            93          -0.22403        0.850\n",
      "            94          -0.22399        0.850\n",
      "            95          -0.22394        0.850\n",
      "            96          -0.22390        0.850\n",
      "            97          -0.22385        0.850\n",
      "            98          -0.22381        0.850\n",
      "            99          -0.22377        0.850\n",
      "         Final          -0.22373        0.850\n",
      "0.794\n",
      "0.7741935483870968\n",
      "  10.183 suffix2=='id' and label is 'female'\n",
      "   9.645 suffix2=='ko' and label is 'female'\n",
      "   9.645 suffix2=='do' and label is 'female'\n",
      "  -9.486 suffix1=='a' and label is 'male'\n",
      "   9.285 suffix2=='vi' and label is 'male'\n",
      "   9.285 suffix2=='ei' and label is 'male'\n",
      "   8.251 suffix2=='ss' and label is 'female'\n",
      "   7.613 suffix2=='sh' and label is 'male'\n",
      "   7.613 suffix2=='ch' and label is 'male'\n",
      "   7.264 suffix2=='rg' and label is 'female'\n"
     ]
    }
   ],
   "source": [
    "me_mod3 = test_classifier(labeled_names, gender_features3, nltk.ConditionalExponentialClassifier)\n",
    "me_mod3.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rapid computation, unclear if reached optimum as continues to improve\n",
    "\n",
    "(Add chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.396\n",
      "             2          -0.61468        0.604\n",
      "             3          -0.58329        0.622\n",
      "             4          -0.55521        0.700\n",
      "             5          -0.53014        0.752\n",
      "             6          -0.50773        0.780\n",
      "             7          -0.48767        0.800\n",
      "             8          -0.46966        0.818\n",
      "             9          -0.45343        0.834\n",
      "            10          -0.43876        0.850\n",
      "            11          -0.42544        0.858\n",
      "            12          -0.41331        0.866\n",
      "            13          -0.40221        0.866\n",
      "            14          -0.39202        0.870\n",
      "            15          -0.38263        0.870\n",
      "            16          -0.37394        0.874\n",
      "            17          -0.36589        0.878\n",
      "            18          -0.35839        0.878\n",
      "            19          -0.35139        0.878\n",
      "            20          -0.34484        0.880\n",
      "            21          -0.33869        0.884\n",
      "            22          -0.33290        0.890\n",
      "            23          -0.32745        0.896\n",
      "            24          -0.32229        0.900\n",
      "            25          -0.31740        0.900\n",
      "            26          -0.31277        0.900\n",
      "            27          -0.30836        0.900\n",
      "            28          -0.30416        0.902\n",
      "            29          -0.30015        0.904\n",
      "            30          -0.29632        0.904\n",
      "            31          -0.29265        0.904\n",
      "            32          -0.28913        0.906\n",
      "            33          -0.28576        0.908\n",
      "            34          -0.28252        0.908\n",
      "            35          -0.27940        0.912\n",
      "            36          -0.27639        0.912\n",
      "            37          -0.27349        0.912\n",
      "            38          -0.27069        0.912\n",
      "            39          -0.26799        0.914\n",
      "            40          -0.26537        0.916\n",
      "            41          -0.26284        0.916\n",
      "            42          -0.26038        0.916\n",
      "            43          -0.25800        0.914\n",
      "            44          -0.25569        0.918\n",
      "            45          -0.25344        0.918\n",
      "            46          -0.25126        0.918\n",
      "            47          -0.24914        0.918\n",
      "            48          -0.24708        0.920\n",
      "            49          -0.24507        0.920\n",
      "            50          -0.24311        0.920\n",
      "            51          -0.24120        0.920\n",
      "            52          -0.23934        0.920\n",
      "            53          -0.23752        0.920\n",
      "            54          -0.23575        0.920\n",
      "            55          -0.23401        0.926\n",
      "            56          -0.23232        0.928\n",
      "            57          -0.23066        0.928\n",
      "            58          -0.22904        0.928\n",
      "            59          -0.22746        0.930\n",
      "            60          -0.22591        0.932\n",
      "            61          -0.22439        0.932\n",
      "            62          -0.22290        0.932\n",
      "            63          -0.22145        0.932\n",
      "            64          -0.22002        0.932\n",
      "            65          -0.21862        0.932\n",
      "            66          -0.21724        0.932\n",
      "            67          -0.21590        0.932\n",
      "            68          -0.21457        0.932\n",
      "            69          -0.21328        0.932\n",
      "            70          -0.21200        0.934\n",
      "            71          -0.21075        0.934\n",
      "            72          -0.20952        0.934\n",
      "            73          -0.20831        0.934\n",
      "            74          -0.20713        0.934\n",
      "            75          -0.20596        0.934\n",
      "            76          -0.20481        0.938\n",
      "            77          -0.20369        0.938\n",
      "            78          -0.20258        0.938\n",
      "            79          -0.20148        0.938\n",
      "            80          -0.20041        0.940\n",
      "            81          -0.19935        0.942\n",
      "            82          -0.19831        0.942\n",
      "            83          -0.19729        0.942\n",
      "            84          -0.19628        0.944\n",
      "            85          -0.19529        0.944\n",
      "            86          -0.19431        0.944\n",
      "            87          -0.19335        0.944\n",
      "            88          -0.19240        0.944\n",
      "            89          -0.19146        0.944\n",
      "            90          -0.19054        0.946\n",
      "            91          -0.18963        0.948\n",
      "            92          -0.18873        0.950\n",
      "            93          -0.18785        0.950\n",
      "            94          -0.18698        0.950\n",
      "            95          -0.18612        0.950\n",
      "            96          -0.18527        0.950\n",
      "            97          -0.18443        0.950\n",
      "            98          -0.18361        0.950\n",
      "            99          -0.18280        0.950\n",
      "         Final          -0.18199        0.950\n",
      "0.76\n",
      "0.7831221198156681\n",
      "   2.871 suffix3=='zra' and label is 'male'\n",
      "   2.808 suffix3=='wen' and label is 'female'\n",
      "   2.504 suffix2=='Bo' and label is 'female'\n",
      "   2.504 suffix3=='Bo' and label is 'female'\n",
      "   2.332 suffix3=='sha' and label is 'male'\n",
      "   2.052 suffix3=='ird' and label is 'female'\n",
      "   2.030 suffix3=='mey' and label is 'male'\n",
      "   2.007 suffix3=='arl' and label is 'male'\n",
      "  -1.924 lastletter=='a' and label is 'male'\n",
      "  -1.924 suffix1=='a' and label is 'male'\n"
     ]
    }
   ],
   "source": [
    "me_mod4 = test_classifier(labeled_names, gender_features4, nltk.ConditionalExponentialClassifier)\n",
    "me_mod4.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
