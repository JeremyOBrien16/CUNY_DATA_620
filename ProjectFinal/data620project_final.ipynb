{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620 - Final Project\n",
    "\n",
    "Jeremy OBrien, Mael Illien, Vanita Thompson\n",
    "\n",
    "## Topic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "A powerful analytical application of NLP is topic modeling, which identifies the themes present in a corpus comprised of multiple documents based on the words in those documents. Because it can be used to uncover the thematic structure in documents, topic modeling has an array of applications in information retrieval and document mining.\n",
    "\n",
    "Topics are 'probability distributions over a fixed vocabulary' 'An overview of topic modeling and its current applications in bioinformatics', and it's common to use probabilistic generative models derived from LDA (Latent Dirichlet Allocation) to model in an unsupervised fashion the latent semantic structure of documents. Topic models can be tuned and optimized in a variety of ways, including improving how topics are segregated from each other and calibrating for a useful number of topics.\n",
    "\n",
    "### Research Question\n",
    "We will combine techniques from topic modeling and network analysis to address this question.\n",
    "\n",
    "Given a text corpus comprised of multiple documents, what are the topics of those documents and how are the documents thematically related to one another?\n",
    "\n",
    "### Approach\n",
    "- Leverage the Reuters news corpus of nearly 11,000 articles (labeled with at least one category each; unfortunately, authorship is not labeled)\n",
    "- Using the NLTK, Spacy, and Gensim packages, implement and tune an unsupervised LDA-based topic model (i.e. without reference to the provided article topic labels)\n",
    "- Analyze model perplexity and coherence, overall topic prevalence, and topic distribution across articles\n",
    "- Generate a bipartite, weighted (likely on coherence) graph of articles and topics, and analyze its topology to identify relationships between topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import spacy  # need to install\n",
    "\n",
    "import gensim  # need to install\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'shipment', 'was', 'for', 'April', '8', 'to', '20', 'delivery', '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Reuters corpus test\n",
    "from nltk.corpus import reuters\n",
    "reuters.fileids()\n",
    "reuters.words('test/14841')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust to generate dataframe directly from nltk corpus\n",
    "# https://stackoverflow.com/questions/46109166/converting-categorizedplaintextcorpusreader-into-dataframe\n",
    "news = []\n",
    "for fileid in reuters.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    news.append((filename, tag, reuters.raw(fileid)))\n",
    "\n",
    "df = pd.DataFrame(news, columns=['filename', 'tag', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14826</td>\n",
       "      <td>test</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14828</td>\n",
       "      <td>test</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14829</td>\n",
       "      <td>test</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14832</td>\n",
       "      <td>test</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14833</td>\n",
       "      <td>test</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename   tag                                               text\n",
       "0    14826  test  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n",
       "1    14828  test  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n",
       "2    14829  test  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n",
       "3    14832  test  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...\n",
       "4    14833  test  INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output with print\n",
    "\n",
    "# 10,788 separate cases (news articles)\n",
    "# df.shape\n",
    "# len(df.filename.unique())\n",
    "\n",
    "# Each case is tagged as either train or test\n",
    "# df.tag.unique()\n",
    "\n",
    "# There are 10,657 articles with distinct text body - so 131 duplicates\n",
    "# len(df.text.unique())\n",
    "\n",
    "# Check the text body of duplicates (except for first instance)\n",
    "# print(df[df.text.duplicated('first')])\n",
    "\n",
    "# JO: Evaluate cause and consider case for removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# JO: Confirm if these / other stopwords should be added\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling & Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
