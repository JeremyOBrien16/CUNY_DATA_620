{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA 620 - Final Project\n",
    "\n",
    "Jeremy OBrien, Mael Illien, Vanita Thompson\n",
    "\n",
    "## Topic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "A powerful analytical application of NLP is topic modeling, which identifies the themes present in a corpus comprised of multiple documents based on the words in those documents. Because it can be used to uncover the thematic structure in documents, topic modeling has an array of applications in information retrieval and document mining.\n",
    "\n",
    "Topics are 'probability distributions over a fixed vocabulary'. An overview of topic modeling and its current applications in bioinformatics, and it's common to use probabilistic generative models derived from LDA (Latent Dirichlet Allocation) to model in an unsupervised fashion the latent semantic structure of documents. Topic models can be tuned and optimized in a variety of ways, including improving how topics are segregated from each other and calibrating for a useful number of topics.\n",
    "\n",
    "### Research Question\n",
    "We will combine techniques from topic modeling and network analysis to address this question.\n",
    "\n",
    "Given a text corpus comprised of multiple documents, what are the topics of those documents and how are the documents thematically related to one another?\n",
    "\n",
    "### Approach\n",
    "- Leverage the Reuters news corpus of nearly 11,000 articles (labeled with at least one category each; unfortunately, authorship is not labeled)\n",
    "- Using the NLTK, Spacy, and Gensim packages, implement and tune an unsupervised LDA-based topic model (i.e. without reference to the provided article topic labels)\n",
    "- Analyze model perplexity and coherence, overall topic prevalence, and topic distribution across articles\n",
    "- Generate a bipartite, weighted (likely on coherence) graph of articles and topics, and analyze its topology to identify relationships between topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "np.random.seed(12321)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "import spacy  # need to install\n",
    "\n",
    "import gensim  # need to install\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "# warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'shipment', 'was', 'for', 'April', '8', 'to', '20', 'delivery', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Reuters corpus test\n",
    "from nltk.corpus import reuters\n",
    "reuters.fileids()\n",
    "reuters.words('test/14841')[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are 25 sampled categoies from the Reuters corpus. These correspond to themes/topics assigned to documents in the corpus and are essentially labels for the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oat',\n",
       " 'gas',\n",
       " 'palmkernel',\n",
       " 'palladium',\n",
       " 'rape-oil',\n",
       " 'orange',\n",
       " 'wheat',\n",
       " 'nat-gas',\n",
       " 'sorghum',\n",
       " 'cpu',\n",
       " 'groundnut-oil',\n",
       " 'housing',\n",
       " 'soybean',\n",
       " 'corn',\n",
       " 'ipi',\n",
       " 'fuel',\n",
       " 'rand',\n",
       " 'barley',\n",
       " 'jet',\n",
       " 'palmkernel',\n",
       " 'pet-chem',\n",
       " 'orange',\n",
       " 'soy-meal',\n",
       " 'coffee',\n",
       " 'money-fx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove\n",
    "random.choices(reuters.categories(),k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust to generate dataframe directly from nltk corpus\n",
    "# https://stackoverflow.com/questions/46109166/converting-categorizedplaintextcorpusreader-into-dataframe\n",
    "news = []\n",
    "for fileid in reuters.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    news.append((filename, tag, reuters.categories(fileid), reuters.raw(fileid)))\n",
    "\n",
    "df = pd.DataFrame(news, columns=['filename', 'tag', 'categories','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>tag</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14826</td>\n",
       "      <td>test</td>\n",
       "      <td>[trade]</td>\n",
       "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14828</td>\n",
       "      <td>test</td>\n",
       "      <td>[grain]</td>\n",
       "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14829</td>\n",
       "      <td>test</td>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14832</td>\n",
       "      <td>test</td>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14833</td>\n",
       "      <td>test</td>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>999</td>\n",
       "      <td>training</td>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>U.K. MONEY MARKET SHORTAGE FORECAST REVISED DO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>9992</td>\n",
       "      <td>training</td>\n",
       "      <td>[earn]</td>\n",
       "      <td>KNIGHT-RIDDER INC &amp;lt;KRN&gt; SETS QUARTERLY\\n  Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>9993</td>\n",
       "      <td>training</td>\n",
       "      <td>[earn]</td>\n",
       "      <td>TECHNITROL INC &amp;lt;TNL&gt; SETS QUARTERLY\\n  Qtly...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>9994</td>\n",
       "      <td>training</td>\n",
       "      <td>[earn]</td>\n",
       "      <td>NATIONWIDE CELLULAR SERVICE INC &amp;lt;NCEL&gt; 4TH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>9995</td>\n",
       "      <td>training</td>\n",
       "      <td>[earn]</td>\n",
       "      <td>&amp;lt;A.H.A. AUTOMOTIVE TECHNOLOGIES CORP&gt; YEAR ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      filename       tag                                      categories  \\\n",
       "0        14826      test                                         [trade]   \n",
       "1        14828      test                                         [grain]   \n",
       "2        14829      test                                [crude, nat-gas]   \n",
       "3        14832      test  [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4        14833      test                             [palm-oil, veg-oil]   \n",
       "...        ...       ...                                             ...   \n",
       "10783      999  training                            [interest, money-fx]   \n",
       "10784     9992  training                                          [earn]   \n",
       "10785     9993  training                                          [earn]   \n",
       "10786     9994  training                                          [earn]   \n",
       "10787     9995  training                                          [earn]   \n",
       "\n",
       "                                                    text  \n",
       "0      ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...  \n",
       "1      CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...  \n",
       "2      JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...  \n",
       "3      THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...  \n",
       "4      INDONESIA SEES CPO PRICE RISING SHARPLY\\n  Ind...  \n",
       "...                                                  ...  \n",
       "10783  U.K. MONEY MARKET SHORTAGE FORECAST REVISED DO...  \n",
       "10784  KNIGHT-RIDDER INC &lt;KRN> SETS QUARTERLY\\n  Q...  \n",
       "10785  TECHNITROL INC &lt;TNL> SETS QUARTERLY\\n  Qtly...  \n",
       "10786  NATIONWIDE CELLULAR SERVICE INC &lt;NCEL> 4TH ...  \n",
       "10787  &lt;A.H.A. AUTOMOTIVE TECHNOLOGIES CORP> YEAR ...  \n",
       "\n",
       "[10788 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide tag column\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output with print\n",
    "# Describe shape of data\n",
    "\n",
    "# 10,788 separate cases (news articles)\n",
    "# df.shape\n",
    "# len(df.filename.unique())\n",
    "\n",
    "# Each case is tagged as either train or test\n",
    "# df.tag.unique()\n",
    "\n",
    "# There are 10,657 articles with distinct text body - so 131 duplicates\n",
    "# len(df.text.unique())\n",
    "\n",
    "# Check the text body of duplicates (except for first instance)\n",
    "# print(df[df.text.duplicated('first')])\n",
    "\n",
    "# JO: Evaluate cause and consider case for removal\n",
    "# MI: I checked with some of the indexes and found no overlap. See below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[df.text.duplicated('first')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('G-7 ISSUES STATEMENT AFTER MEETING Following is the text of a statement by '\n",
      " 'the Group of Seven -- the U.S., Japan, West Germany, France, Britain, Italy '\n",
      " 'and Canada -- issued after a Washington meeting yesterday. 1. The finance '\n",
      " 'ministers and central bank governors of seven major industrial countries met '\n",
      " 'today. They continued the process of multilateral surveillance of their '\n",
      " 'economies pursuant to the arrangements for strengthened economic policy '\n",
      " 'coordination agreed at the 1986 Tokyo summit of their heads of state or '\n",
      " 'government. The managing director of the International Monetary Fund also '\n",
      " 'participated in the meeting. 2. The ministers and governors reaffirmed the '\n",
      " 'commitment to the cooperative approach agreed at the recent Paris meeting, '\n",
      " 'and noted the progress achieved in implementing the undertakings embodied in '\n",
      " 'the Louvre Agreement. They agreed, however, that further actions will be '\n",
      " 'essential to resist rising protectionist pressures, sustain global economic '\n",
      " 'expansion, and reduce trade imba')\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df['text'].values.tolist()\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r'\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(r\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[330][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JAPAN BUSINESS LEADERS SAY G-7 ACCORD IS WORRYING The leaders of two of '\n",
      " 'Japans top business groups said in separate statements the Group of Seven '\n",
      " '(G-7) accord reached in Washington yesterday is of deep concern to Japan '\n",
      " 'because it shows the major industrial nations regard the yens current level '\n",
      " 'as appropriate. Eishiro Saito, chairman of the Federation of Economic '\n",
      " 'Organizations (Keidanren), said the yens present rate is well above adequate '\n",
      " 'levels. He did not elaborate. Takashi Ishihara, chairman of the Japan '\n",
      " 'Committee for Economic Development, said the accord will not prevent the yen '\n",
      " 'from rising further. \"We do not understand why the G-7 approved present '\n",
      " 'rates as the yen has risen excessively since the Paris accord,\" Ishihara '\n",
      " 'said. G-7 members Britain, Canada, France, Italy, Japan, the U.S. And West '\n",
      " 'Germany said in a statement they consider their currencies are now within '\n",
      " 'ranges broadly consistent with economic fundamentals. Saito called on each '\n",
      " 'G-7 member nation to prepare to intervene ')\n"
     ]
    }
   ],
   "source": [
    "# only one example\n",
    "pprint(data[333][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asias', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyos', 'loss', 'might', 'be', 'their', 'gain', 'the', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japans', 'alleged', 'failure', 'to', 'stick', 'to', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', 'we', 'wouldnt', 'be', 'able', 'to', 'do', 'business', 'said', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'said', 'tom', 'murtha', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', 'in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'warning', 'to', 'us', 'said', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', 'taiwan', 'had', 'trade', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'last', 'year', 'pct', 'of', 'it', 'with', 'the', 'the', 'surplus', 'helped', 'swell', 'taiwans', 'foreign', 'exchange', 'reserves', 'to', 'billion', 'dlrs', 'among', 'the', 'worlds', 'largest', 'we', 'must', 'quickly', 'open', 'our', 'markets', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'products', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'retaliation', 'said', 'paul', 'sheen', 'chairman', 'of', 'textile', 'exporters', 'lt', 'taiwan', 'safe', 'group', 'senior', 'official', 'of', 'south', 'koreas', 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', 'last', 'year', 'south', 'korea', 'had', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'with', 'the', 'up', 'from', 'billion', 'dlrs', 'in', 'in', 'malaysia', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'hard', 'hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'in', 'hong', 'kong', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below', 'cost', 'semiconductors', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', 'but', 'other', 'businessmen', 'said', 'such', 'short', 'term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'pressure', 'to', 'block', 'imports', 'that', 'is', 'very', 'short', 'term', 'view', 'said', 'lawrence', 'mills', 'director', 'general', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry', 'if', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports', 'one', 'day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources', 'much', 'more', 'serious', 'for', 'hong', 'kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining', 'trade', 'he', 'said', 'the', 'last', 'year', 'was', 'hong', 'kongs', 'biggest', 'export', 'market', 'accounting', 'for', 'over', 'pct', 'of', 'domestically', 'produced', 'exports', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'and', 'japan', 'with', 'interest', 'and', 'concern', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday', 'this', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two', 'countries', 'which', 'are', 'major', 'trading', 'partners', 'of', 'ours', 'is', 'very', 'serious', 'matter', 'button', 'said', 'he', 'said', 'australias', 'concerns', 'centred', 'on', 'coal', 'and', 'beef', 'australias', 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'exports', 'to', 'that', 'country', 'meanwhile', 'japanese', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'stand', 'off', 'continue', 'japans', 'ruling', 'liberal', 'democratic', 'party', 'yesterday', 'outlined', 'package', 'of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy', 'the', 'measures', 'proposed', 'include', 'large', 'supplementary', 'budget', 'and', 'record', 'public', 'works', 'spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year', 'they', 'also', 'call', 'for', 'stepped', 'up', 'spending', 'as', 'an', 'emergency', 'measure', 'to', 'stimulate', 'the', 'economy', 'despite', 'prime', 'minister', 'yasuhiro', 'nakasones', 'avowed', 'fiscal', 'reform', 'program', 'deputy', 'trade', 'representative', 'michael', 'smith', 'and', 'makoto', 'kuroda', 'japans', 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this', 'week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words and clean up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "#truncate\n",
    "print(data_words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporters', 'fear', 'damage', 'from', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asias', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'they', 'told', 'reuter_correspondents', 'in', 'asian', 'capitals', 'move', 'against', 'japan', 'might', 'boost', 'protectionist_sentiment', 'in', 'the', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short_term', 'tokyos', 'loss', 'might', 'be', 'their', 'gain', 'the', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japans', 'alleged_failure', 'to', 'stick', 'to', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', 'we', 'wouldnt', 'be', 'able', 'to', 'do', 'business', 'said', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'said', 'tom', 'murtha', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james_capel', 'and', 'co', 'in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'warning', 'to', 'us', 'said', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', 'taiwan', 'had', 'trade', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'last', 'year', 'pct', 'of', 'it', 'with', 'the', 'the', 'surplus', 'helped', 'swell', 'taiwans', 'foreign', 'exchange', 'reserves', 'to', 'billion', 'dlrs', 'among', 'the', 'worlds_largest', 'we', 'must', 'quickly', 'open', 'our', 'markets', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'products', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'retaliation', 'said', 'paul', 'sheen', 'chairman', 'of', 'textile', 'exporters', 'lt', 'taiwan', 'safe', 'group', 'senior', 'official', 'of', 'south_koreas', 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south_korea', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', 'last', 'year', 'south_korea', 'had', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'with', 'the', 'up', 'from', 'billion', 'dlrs', 'in', 'in', 'malaysia', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'hard_hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'in', 'hong_kong', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below', 'cost', 'semiconductors', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', 'but', 'other', 'businessmen', 'said', 'such', 'short_term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'pressure', 'to', 'block', 'imports', 'that', 'is', 'very', 'short_term', 'view', 'said', 'lawrence', 'mills', 'director', 'general', 'of', 'the', 'federation', 'of', 'hong_kong', 'industry', 'if', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports', 'one', 'day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources', 'much', 'more', 'serious', 'for', 'hong_kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining', 'trade', 'he', 'said', 'the', 'last', 'year', 'was', 'hong_kongs', 'biggest', 'export', 'market', 'accounting', 'for', 'over', 'pct', 'of', 'domestically_produced', 'exports', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'and', 'japan', 'with', 'interest', 'and', 'concern', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday', 'this', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two', 'countries', 'which', 'are', 'major', 'trading_partners', 'of', 'ours', 'is', 'very', 'serious', 'matter', 'button', 'said', 'he', 'said', 'australias', 'concerns', 'centred', 'on', 'coal', 'and', 'beef', 'australias', 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'exports', 'to', 'that', 'country', 'meanwhile', 'japanese', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'stand', 'off', 'continue', 'japans', 'ruling_liberal_democratic_party', 'yesterday', 'outlined', 'package', 'of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy', 'the', 'measures', 'proposed', 'include', 'large', 'supplementary', 'budget', 'and', 'record', 'public_works', 'spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year', 'they', 'also', 'call', 'for', 'stepped', 'up', 'spending', 'as', 'an', 'emergency', 'measure', 'to', 'stimulate', 'the', 'economy', 'despite', 'prime_minister_yasuhiro_nakasones', 'avowed', 'fiscal', 'reform', 'program', 'deputy', 'trade', 'representative_michael', 'smith', 'and', 'makoto_kuroda', 'japans', 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this', 'week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# JO: Confirm if these / other stopwords should be added\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asian', 'exporter', 'fear', 'damage', 'mount', 'trade', 'friction', 'raise', 'fear', 'many', 'asia', 'export', 'nation', 'row', 'could', 'inflict', 'far', 'reach', 'economic', 'damage', 'businessman', 'official', 'say', 'tell', 'asian', 'capital', 'move', 'may', 'boost', 'protectionist_sentiment', 'lead', 'curb', 'american', 'import', 'product', 'exporter', 'say', 'conflict', 'would', 'hurt', 'long', 'run', 'tokyos', 'loss', 'may', 'gain', 'say', 'impose', 'dlrs', 'import', 'japanese', 'electronic', 'good', 'retaliation', 'stick', 'pact', 'sell', 'semiconductor', 'world', 'market', 'cost', 'unofficial', 'japanese', 'estimate', 'put', 'impact', 'tariff', 'dlrs', 'spokesman', 'major', 'electronic', 'firm', 'say', 'would', 'virtually', 'halt', 'export', 'product', 'hit', 'new', 'taxis', 'would', 'able', 'business', 'say', 'spokesman', 'lead', 'japanese', 'electronic', 'firm', 'tariff', 'remain', 'place', 'length', 'time', 'month', 'mean', 'complete', 'erosion', 'export', 'good', 'tariff', 'say', 'official', 'also', 'worry', 'aware', 'seriousness', 'threat', 'serve', 'warn', 'say', 'senior', 'taiwanese', 'trade', 'official', 'ask', 'name', 'trade', 'surplus', 'dlrs', 'last', 'year', 'surplus', 'help', 'swell', 'foreign', 'exchange', 'reserve', 'dlrs', 'must', 'quickly', 'open', 'market', 'remove', 'trade', 'barrier', 'cut', 'import', 'tariff', 'allow', 'import', 'product', 'want', 'defuse', 'problem', 'possible', 'retaliation', 'say', 'senior', 'official', 'say', 'dispute', 'may', 'also', 'lead', 'pressure', 'chief', 'export', 'similar', 'last', 'year', 'trade', 'surplus', 'officer', 'businessman', 'say', 'tough', 'may', 'allow', 'producer', 'semiconductor', 'third', 'country', 'expand', 'sale', 'newspaper', 'allege', 'sell', 'cost', 'semiconductor', 'electronic', 'manufacturer', 'share', 'view', 'businessman', 'say', 'commercial', 'advantage', 'would', 'outweighed', 'pressure', 'block', 'import', 'say', 'industry', 'whole', 'purpose', 'prevent', 'import', 'day', 'extended', 'source', 'much', 'serious', 'disadvantage', 'action', 'restrain', 'trade', 'say', 'last', 'year', 'big', 'export', 'market', 'accounting', 'domestically_produced', 'export', 'australian', 'government', 'await', 'talk', 'interest', 'concern', 'say', 'last', 'kind', 'deterioration', 'trade', 'relation', 'country', 'major', 'trading', 'partner', 'serious', 'matter', 'say', 'say', 'concern', 'centre', 'coal', 'beef', 'australia', 'large', 'export', 'also', 'significant', 'export', 'country', 'meanwhile', 'japanese', 'diplomatic', 'manoeuvre', 'solve', 'trade', 'stand', 'continue', 'yesterday', 'outline', 'package', 'economic', 'measure', 'boost', 'japanese', 'economy', 'measure', 'propose', 'include', 'large', 'supplementary', 'budget', 'record', 'public_work', 'spend', 'first', 'financial', 'year', 'also', 'call', 'step', 'spend', 'emergency', 'measure', 'stimulate', 'economy', 'yasuhiro_nakasone', 'avow', 'fiscal', 'reform', 'program', 'deputy', 'trade', 'representative', 'industry', 'due', 'week', 'effort', 'end', 'dispute']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 4), (7, 1), (8, 1), (9, 2), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 3), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 2), (32, 1), (33, 1), (34, 2), (35, 1), (36, 3), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 2), (47, 4), (48, 1), (49, 1), (50, 2), (51, 2), (52, 1), (53, 4), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 8), (61, 2), (62, 1), (63, 1), (64, 2), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 2), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 6), (80, 1), (81, 1), (82, 2), (83, 1), (84, 1), (85, 5), (86, 1), (87, 2), (88, 4), (89, 3), (90, 1), (91, 1), (92, 1), (93, 2), (94, 1), (95, 1), (96, 1), (97, 3), (98, 1), (99, 4), (100, 1), (101, 1), (102, 3), (103, 1), (104, 1), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 4), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 1), (121, 1), (122, 2), (123, 1), (124, 1), (125, 1), (126, 3), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 2), (145, 1), (146, 1), (147, 1), (148, 16), (149, 2), (150, 3), (151, 2), (152, 2), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 2), (161, 2), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 3), (168, 1), (169, 1), (170, 1), (171, 4), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 9), (180, 1), (181, 1), (182, 1), (183, 1), (184, 1), (185, 1), (186, 1), (187, 1), (188, 1), (189, 1), (190, 4), (191, 1), (192, 4), (193, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create dictionary and corpus for topic modeling\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that bigram such as 'domestically_produced' and 'protectionist_sentiment' are included in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('able', 1),\n",
       "  ('accounting', 1),\n",
       "  ('action', 1),\n",
       "  ('advantage', 1),\n",
       "  ('allege', 1),\n",
       "  ('allow', 2),\n",
       "  ('also', 4),\n",
       "  ('american', 1),\n",
       "  ('asia', 1),\n",
       "  ('asian', 2),\n",
       "  ('ask', 1),\n",
       "  ('australia', 1),\n",
       "  ('australian', 1),\n",
       "  ('avow', 1),\n",
       "  ('await', 1),\n",
       "  ('aware', 1),\n",
       "  ('barrier', 1),\n",
       "  ('beef', 1),\n",
       "  ('big', 1),\n",
       "  ('block', 1),\n",
       "  ('boost', 2),\n",
       "  ('budget', 1),\n",
       "  ('business', 1),\n",
       "  ('businessman', 3),\n",
       "  ('call', 1),\n",
       "  ('capital', 1),\n",
       "  ('centre', 1),\n",
       "  ('chief', 1),\n",
       "  ('coal', 1),\n",
       "  ('commercial', 1),\n",
       "  ('complete', 1),\n",
       "  ('concern', 2),\n",
       "  ('conflict', 1),\n",
       "  ('continue', 1),\n",
       "  ('cost', 2),\n",
       "  ('could', 1),\n",
       "  ('country', 3),\n",
       "  ('curb', 1),\n",
       "  ('cut', 1),\n",
       "  ('damage', 2),\n",
       "  ('day', 1),\n",
       "  ('defuse', 1),\n",
       "  ('deputy', 1),\n",
       "  ('deterioration', 1),\n",
       "  ('diplomatic', 1),\n",
       "  ('disadvantage', 1),\n",
       "  ('dispute', 2),\n",
       "  ('dlrs', 4),\n",
       "  ('domestically_produced', 1),\n",
       "  ('due', 1),\n",
       "  ('economic', 2),\n",
       "  ('economy', 2),\n",
       "  ('effort', 1),\n",
       "  ('electronic', 4),\n",
       "  ('emergency', 1),\n",
       "  ('end', 1),\n",
       "  ('erosion', 1),\n",
       "  ('estimate', 1),\n",
       "  ('exchange', 1),\n",
       "  ('expand', 1),\n",
       "  ('export', 8),\n",
       "  ('exporter', 2),\n",
       "  ('extended', 1),\n",
       "  ('far', 1),\n",
       "  ('fear', 2),\n",
       "  ('financial', 1),\n",
       "  ('firm', 2),\n",
       "  ('first', 1),\n",
       "  ('fiscal', 1),\n",
       "  ('foreign', 1),\n",
       "  ('friction', 1),\n",
       "  ('gain', 1),\n",
       "  ('good', 2),\n",
       "  ('government', 1),\n",
       "  ('halt', 1),\n",
       "  ('help', 1),\n",
       "  ('hit', 1),\n",
       "  ('hurt', 1),\n",
       "  ('impact', 1),\n",
       "  ('import', 6),\n",
       "  ('impose', 1),\n",
       "  ('include', 1),\n",
       "  ('industry', 2),\n",
       "  ('inflict', 1),\n",
       "  ('interest', 1),\n",
       "  ('japanese', 5),\n",
       "  ('kind', 1),\n",
       "  ('large', 2),\n",
       "  ('last', 4),\n",
       "  ('lead', 3),\n",
       "  ('length', 1),\n",
       "  ('long', 1),\n",
       "  ('loss', 1),\n",
       "  ('major', 2),\n",
       "  ('manoeuvre', 1),\n",
       "  ('manufacturer', 1),\n",
       "  ('many', 1),\n",
       "  ('market', 3),\n",
       "  ('matter', 1),\n",
       "  ('may', 4),\n",
       "  ('mean', 1),\n",
       "  ('meanwhile', 1),\n",
       "  ('measure', 3),\n",
       "  ('month', 1),\n",
       "  ('mount', 1),\n",
       "  ('move', 1),\n",
       "  ('much', 1),\n",
       "  ('must', 1),\n",
       "  ('name', 1),\n",
       "  ('nation', 1),\n",
       "  ('new', 1),\n",
       "  ('newspaper', 1),\n",
       "  ('officer', 1),\n",
       "  ('official', 4),\n",
       "  ('open', 1),\n",
       "  ('outline', 1),\n",
       "  ('outweighed', 1),\n",
       "  ('package', 1),\n",
       "  ('pact', 1),\n",
       "  ('partner', 1),\n",
       "  ('place', 1),\n",
       "  ('possible', 1),\n",
       "  ('pressure', 2),\n",
       "  ('prevent', 1),\n",
       "  ('problem', 1),\n",
       "  ('producer', 1),\n",
       "  ('product', 3),\n",
       "  ('program', 1),\n",
       "  ('propose', 1),\n",
       "  ('protectionist_sentiment', 1),\n",
       "  ('public_work', 1),\n",
       "  ('purpose', 1),\n",
       "  ('put', 1),\n",
       "  ('quickly', 1),\n",
       "  ('raise', 1),\n",
       "  ('reach', 1),\n",
       "  ('record', 1),\n",
       "  ('reform', 1),\n",
       "  ('relation', 1),\n",
       "  ('remain', 1),\n",
       "  ('remove', 1),\n",
       "  ('representative', 1),\n",
       "  ('reserve', 1),\n",
       "  ('restrain', 1),\n",
       "  ('retaliation', 2),\n",
       "  ('row', 1),\n",
       "  ('run', 1),\n",
       "  ('sale', 1),\n",
       "  ('say', 16),\n",
       "  ('sell', 2),\n",
       "  ('semiconductor', 3),\n",
       "  ('senior', 2),\n",
       "  ('serious', 2),\n",
       "  ('seriousness', 1),\n",
       "  ('serve', 1),\n",
       "  ('share', 1),\n",
       "  ('significant', 1),\n",
       "  ('similar', 1),\n",
       "  ('solve', 1),\n",
       "  ('source', 1),\n",
       "  ('spend', 2),\n",
       "  ('spokesman', 2),\n",
       "  ('stand', 1),\n",
       "  ('step', 1),\n",
       "  ('stick', 1),\n",
       "  ('stimulate', 1),\n",
       "  ('supplementary', 1),\n",
       "  ('surplus', 3),\n",
       "  ('swell', 1),\n",
       "  ('taiwanese', 1),\n",
       "  ('talk', 1),\n",
       "  ('tariff', 4),\n",
       "  ('taxis', 1),\n",
       "  ('tell', 1),\n",
       "  ('third', 1),\n",
       "  ('threat', 1),\n",
       "  ('time', 1),\n",
       "  ('tokyos', 1),\n",
       "  ('tough', 1),\n",
       "  ('trade', 9),\n",
       "  ('trading', 1),\n",
       "  ('unofficial', 1),\n",
       "  ('view', 1),\n",
       "  ('virtually', 1),\n",
       "  ('want', 1),\n",
       "  ('warn', 1),\n",
       "  ('week', 1),\n",
       "  ('whole', 1),\n",
       "  ('world', 1),\n",
       "  ('worry', 1),\n",
       "  ('would', 4),\n",
       "  ('yasuhiro_nakasone', 1),\n",
       "  ('year', 4),\n",
       "  ('yesterday', 1)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a topic model for the article.  We employ two approaches:\n",
    "\n",
    "1. Gensim's built-in LDA topic modeling function, `ldamodel`.  Gensim implementations of LDA employ Variational Inference, which introduces bias and can provide less coherence in topics.\n",
    "2. MALLET's LDA topic model with a Gensim wrapper, `ldamallet`.  This model employs Gibbs sampling (an MCMC method), which introduces is unbiased at the cost of heavy computation (performed in Java)\n",
    "\n",
    "LDA models can be evaluated based on several different metrics:\n",
    "\n",
    "- **[Perplexity](https://en.wikipedia.org/wiki/Perplexity)** is a measure of 'surprise', or how well the model predicts.  The lower the score, the better the prediction.\n",
    "- Coherence is the degree of 'semantic similarity between high scoring keywords in the topic' [Learn to Find Topics in a Text Corpus](https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0).  It is computed as a the sum of pairwise scores of the top n words by frequency.  The higher the coherence, the more coherent and interpretable the topic. \n",
    "\n",
    "Selecting the number of topics in an LDA model can be an arbitrary decision. We inform this decision through hyperparameter tuning based on an evaluation of the topic coherence yielded by different numbers of topics.  We optimize the Gensim model accordingly (JO: MALLET too?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to iterate over numbers of topics and calculates LDA model coherence\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "   \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    coherence='c_v'\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        if model_type == 'LdaModel':\n",
    "        \n",
    "            model = gensim.models.ldamodel.LdaModel(corpus=corpus,  # TDF corpus\n",
    "                                               id2word=id2word,  # dictionary\n",
    "                                               num_topics=num_topics,  # to be optimized\n",
    "                                               random_state=100,  # [JO: CONFIRM]\n",
    "                                               # update_every=1,  # how often paramater should be updated\n",
    "                                               # chunksize=100,\n",
    "                                               # passes=10,  # total number of training passes\n",
    "                                               # alpha='auto',  # learn assymetric alpha from training data (JO: consider whether to evaluate symmetric approach for alpha with asymmetric for beta)\n",
    "                                               per_word_topics=True)\n",
    "        \n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=model, \n",
    "                                            texts=texts, \n",
    "                                            dictionary=dictionary, \n",
    "                                            coherence=coherence)\n",
    "            coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "        if model_type == 'LdaMallet':\n",
    "            \n",
    "            mallet_path = './mallet-2.0.8/bin/mallet' \n",
    "            \n",
    "            model = gensim.models.wrappers.LdaMallet(mallet_path, \n",
    "                                                     corpus=corpus, \n",
    "                                                     num_topics=num_topics, \n",
    "                                                     id2word=id2word)\n",
    "            \n",
    "            model_list.append(model)\n",
    "            coherencemodel = CoherenceModel(model=ldamallet, \n",
    "                                                texts=texts,\n",
    "                                                dictionary=id2word, \n",
    "                                                coherence=coherence)\n",
    "            cohernece_values.append(coherencemodel.get_coherence())\n",
    "        \n",
    "    return model_list, coherence_values  # List of LDA topic models and coherence values for respective number of topics\n",
    "\n",
    "# Function to support decision on optimal number of topics\n",
    "def chart_model_coherence(coherence_values, limit, start=2, step=3):\n",
    "    \n",
    "    x = range(start, limit, step)\n",
    "    \n",
    "    coherence_table = pd.DataFrame(list(zip(x,coherence_values)), columns=['Num Topics','Coherence Score'])\n",
    "    \n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel('Num Topics')\n",
    "    plt.ylabel('Coherence Score')\n",
    "    plt.legend(('coherence_values'), loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return coherence_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast broad net to find neighborhood of optimal number of topics in range of 2 to 40 in steps of 4\n",
    "gs_broad_model_list, gs_broad_coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                                        corpus=corpus, \n",
    "                                                                        texts=data_lemmatized, \n",
    "                                                                        start=2, \n",
    "                                                                        limit=40, \n",
    "                                                                        step=4)\n",
    "\n",
    "# Chart broad net of optimal number of topics\n",
    "chart_model_coherence(gs_broad_coherence_values, \n",
    "                      start=2, \n",
    "                      limit=40, \n",
    "                      step=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzed at 4 unit increments, Gensim LDA model coherence peaks between 6 and 25 topics.  It's not very smooth, so given the spikes we narrow our search in that range to find the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast narrower net in range of 6 to 25 in single steps to smooth and find optimum in number of topics\n",
    "gs_narrow_model_list, gs_narrow_coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=data_lemmatized, \n",
    "                                                        start=6, \n",
    "                                                        limit=25, \n",
    "                                                        step=1)\n",
    "\n",
    "# Chart narrower net of optimal number of topics\n",
    "gs_narrow_coherence = chart_model_coherence(gs_narrow_coherence_values, \n",
    "                      start=8, \n",
    "                      limit=25, \n",
    "                      step=1)     \n",
    "gs_narrow_coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 topics yield the highest coherence score.  We select the Gensim model with 9 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Gensim model\n",
    "gs_optimal_model = gs_narrow_model_list[int(gs_narrow_coherence.loc[gs_narrow_coherence['Coherence Score'].idxmax()]['Num Topics'])]\n",
    "\n",
    "# Compute model perplexity and coherence score\n",
    "print('\\nGensim LDA model perplexity: ', lda_model.log_perplexity(corpus))\n",
    "print('\\nGensim LDA model coherence score: ', gs_narrow_coherence.loc[gs_narrow_coherence['Coherence Score'].idxmax()]['Coherence Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a quick look at the the top words by topic for the optimal Gensim model\n",
    "for topic_id in range(gs_optimal_model.num_topics):\n",
    "    top_k = optimal_model.show_topic(topic_id, 10)\n",
    "    top_k_words = [w for w, _ in top_k]\n",
    "    \n",
    "    print('{}: {}'.format(topic_id, ' '.join(top_k_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MALLET LDA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JO: Mael, what if we trained both models first in this section, and then compared their relative performance before selecting MALLET and analyzing its output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Prevalence and Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JO: troubleshoot TypeError:  https://cs50.stackexchange.com/questions/30671/typeerror-not-supported-between-instances-of-int-and-tuple\n",
    "\n",
    "\n",
    "# MI: this is now included in the best model functions.\n",
    "\n",
    "# Find dominant topic by sentence\n",
    "# def format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "#     # Init output\n",
    "#     sent_topics_df = pd.DataFrame()\n",
    "\n",
    "#     # Loop through articles to get each article's main topic\n",
    "#     for i, article in enumerate(ldamodel[corpus]):\n",
    "#         for article in enumerate(article):  # JO: removed the sort operation here - think we'll need to figure out another way to resort the article value, but leaving for now\n",
    "#             if article[0] == 0:\n",
    "                \n",
    "#                 print(article)\n",
    "                \n",
    "#                 wp = optimal_model.show_topic(article[1][0][0])\n",
    "#                 topic_keywords = ', '.join([word for word, prop in wp])\n",
    "#                 sent_topics_df = sent_topics_df.append(pd.Series([int(article[1][0][0]), \n",
    "#                                                                   round(article[1][0][1],4), \n",
    "#                                                                   topic_keywords]), \n",
    "#                                                        ignore_index=True)\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "#     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "#     return(sent_topics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "# df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords']\n",
    "\n",
    "# Show\n",
    "#df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will need to be adjusted for Gensim rather than MALLET\n",
    "\n",
    "# Find most representative document by topic\n",
    "# Group top 5 sentences under each topic\n",
    "# sent_topics_sorted_df = pd.DataFrame()\n",
    "\n",
    "# sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "# for i, grp in sent_topics_outdf_grpd:\n",
    "#     sent_topics_sorted_df = pd.concat([sent_topics_sorted_df, \n",
    "#                                              grp.sort_values(['Perc_Contribution'], \n",
    "#                                                              ascending=[0]).head(1)], \n",
    "#                                             axis=0)\n",
    "\n",
    "# # Reset Index    \n",
    "# sent_topics_sorted_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Format\n",
    "# sent_topics_sorted_df.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\"]\n",
    "\n",
    "# # Show\n",
    "# sent_topics_sorted_df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_topics_sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depends on preceding code which needs troubleshooting\n",
    "# Will need to be adjusted for Gensim rather than MALLET\n",
    "\n",
    "# Evaluate topic distribution across documents\n",
    "\n",
    "# # Number of documents by topic\n",
    "# topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# # Percentage of documents by topic\n",
    "# topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# # Topic number and keywords\n",
    "# topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# # Concatenate columns\n",
    "# df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# # Set column names\n",
    "# df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JO: assuming MALLET is the top performer, do this for MALLET ony?\n",
    "\n",
    "# Visualize the topics\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling using MALLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Build Mallet optimal model directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7,\n",
      "  [('company', 0.09249665029030818),\n",
      "   ('sell', 0.05600714604734256),\n",
      "   ('unit', 0.05462259937472086),\n",
      "   ('buy', 0.04917373827601608),\n",
      "   ('group', 0.041715051362215275),\n",
      "   ('business', 0.03729343456900402),\n",
      "   ('firm', 0.028584189370254576),\n",
      "   ('investment', 0.027020991514068782),\n",
      "   ('product', 0.018088432335864227),\n",
      "   ('acquire', 0.0150960250111657)]),\n",
      " (19,\n",
      "  [('spokesman', 0.041512960503664355),\n",
      "   ('today', 0.021887757611529193),\n",
      "   ('pay', 0.01844473956027741),\n",
      "   ('ship', 0.017559392061384092),\n",
      "   ('strike', 0.015247651369829324),\n",
      "   ('begin', 0.013968816093650091),\n",
      "   ('state', 0.01391963012148935),\n",
      "   ('close', 0.013821258177167872),\n",
      "   ('work', 0.013772072205007133),\n",
      "   ('port', 0.011017657764005706)]),\n",
      " (18,\n",
      "  [('rate', 0.14285069710779227),\n",
      "   ('interest', 0.05937824301764202),\n",
      "   ('cut', 0.03794612642692776),\n",
      "   ('week', 0.03343410188151424),\n",
      "   ('day', 0.022289401254342825),\n",
      "   ('credit', 0.020755312908902226),\n",
      "   ('bank', 0.01822857916347065),\n",
      "   ('point', 0.017957857690745836),\n",
      "   ('note', 0.016198168118034564),\n",
      "   ('month', 0.01583720615440148)]),\n",
      " (10,\n",
      "  [('official', 0.08950563746747615),\n",
      "   ('japanese', 0.036773633998265394),\n",
      "   ('government', 0.028404163052905464),\n",
      "   ('source', 0.026366001734605377),\n",
      "   ('industry', 0.021335646140503036),\n",
      "   ('plan', 0.01725932350390286),\n",
      "   ('market', 0.013746747614917607),\n",
      "   ('foreign', 0.013356461405030355),\n",
      "   ('firm', 0.011795316565481353),\n",
      "   ('open', 0.010407632263660017)]),\n",
      " (6,\n",
      "  [('company', 0.06348475076015848),\n",
      "   ('loan', 0.03685616880125311),\n",
      "   ('asset', 0.03432230719616696),\n",
      "   ('acquisition', 0.031419883903068276),\n",
      "   ('merger', 0.02870174145397586),\n",
      "   ('complete', 0.026213950059891276),\n",
      "   ('debt', 0.025154335206855248),\n",
      "   ('agreement', 0.02473970330784115),\n",
      "   ('term', 0.02308117571178476),\n",
      "   ('acquire', 0.022205841702754997)]),\n",
      " (12,\n",
      "  [('growth', 0.042661230541141584),\n",
      "   ('government', 0.03458117123795404),\n",
      "   ('economic', 0.027575982209043736),\n",
      "   ('year', 0.026945885841363974),\n",
      "   ('economy', 0.02468495181616012),\n",
      "   ('deficit', 0.01793921423276501),\n",
      "   ('grow', 0.01608598962194218),\n",
      "   ('budget', 0.015011119347664937),\n",
      "   ('inflation', 0.014195700518902891),\n",
      "   ('increase', 0.013232023721275018)]),\n",
      " (1,\n",
      "  [('rise', 0.1408339544876573),\n",
      "   ('fall', 0.078002367694129),\n",
      "   ('year', 0.05007234620949708),\n",
      "   ('increase', 0.049853115271627134),\n",
      "   ('compare', 0.026877712982856142),\n",
      "   ('month', 0.023282325601788924),\n",
      "   ('show', 0.022536940413031087),\n",
      "   ('drop', 0.02183540141184724),\n",
      "   ('decline', 0.02174770903669926),\n",
      "   ('figure', 0.0206515543473495)]),\n",
      " (3,\n",
      "  [('dollar', 0.06696882081737908),\n",
      "   ('bank', 0.06649996092834258),\n",
      "   ('market', 0.06427287645541924),\n",
      "   ('exchange', 0.043603969680393845),\n",
      "   ('currency', 0.03707900289130265),\n",
      "   ('foreign', 0.029460029694459638),\n",
      "   ('yen', 0.02750644682347425),\n",
      "   ('mark', 0.026021723841525356),\n",
      "   ('dealer', 0.02567007892474799),\n",
      "   ('money', 0.02492771743377354)]),\n",
      " (5,\n",
      "  [('offer', 0.10245882297418472),\n",
      "   ('make', 0.0378970220397376),\n",
      "   ('bid', 0.027467081976497238),\n",
      "   ('tender', 0.026428807400066073),\n",
      "   ('hold', 0.023833120958988156),\n",
      "   ('takeover', 0.01698994761432819),\n",
      "   ('buy', 0.016470810326112605),\n",
      "   ('purchase', 0.015762895842182264),\n",
      "   ('give', 0.01538534145075275),\n",
      "   ('seek', 0.014677426966822407)]),\n",
      " (8,\n",
      "  [('price', 0.06822837801727977),\n",
      "   ('producer', 0.03415872450342923),\n",
      "   ('agreement', 0.024717199608087645),\n",
      "   ('coffee', 0.023692883227932664),\n",
      "   ('meeting', 0.022045069920726818),\n",
      "   ('member', 0.02124343101451857),\n",
      "   ('quota', 0.02093168255099314),\n",
      "   ('talk', 0.018838514295893827),\n",
      "   ('consumer', 0.017368842967845372),\n",
      "   ('market', 0.017012559009530595)])]\n"
     ]
    }
   ],
   "source": [
    "# Build LDA Mallet model\n",
    "mallet_path = './mallet-2.0.8/bin/mallet' \n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=id2word)\n",
    "\n",
    "# Show topics\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence score:  0.5022628060961075\n"
     ]
    }
   ],
   "source": [
    "# Compute coherence score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, \n",
    "                                            texts=data_lemmatized,\n",
    "                                            dictionary=id2word, \n",
    "                                            coherence='c_v')\n",
    "\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Optimal Mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "\n",
    "# MI\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# vis = pyLDAvis.gensim.prepare(ldamallet, corpus, id2word)\n",
    "# vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best topic for each article along with associate keywords. \n",
    "\n",
    "# MI: Turned this into two functions, one for mallet, one for gensim. This can be removed\n",
    "\n",
    "# art_topic_keywords = pd.DataFrame(columns = ['article','best_topic','likelyhood','keywords'])\n",
    "\n",
    "# for i in range(len(lda_mallet_corpus)):\n",
    "#     article = lda_mallet_corpus[i]\n",
    "    \n",
    "#     #print(\"Article \" + str(i))\n",
    "    \n",
    "#     # Find the max likelyhood index and select the corresponding topic\n",
    "#     likelyhood = [likelyhood for topic,likelyhood in article]\n",
    "#     max_likelyhood = max(likelyhood)\n",
    "#     best_topic = likelyhood.index(max_likelyhood)\n",
    "    \n",
    "#     #print('most likely topic: {}, likelyhood: {}'.format(best_topic, max_likelyhood))\n",
    "    \n",
    "#     wp = ldamallet.show_topic(best_topic)\n",
    "#     #pprint(wp)\n",
    "#     topic_keywords = ', '.join([word for word, prop in wp])\n",
    "#     #print(topic_keywords)\n",
    "#     d = {'article': i, 'best_topic': best_topic,'likelyhood': max_likelyhood,'keywords': topic_keywords}\n",
    "#     art_topic_keywords = art_topic_keywords.append(d, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Gensim Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best topic for each article along with associate keywords and likelyhood\n",
    "def get_best_gensim_topics(lda_model, corpus):\n",
    "    \n",
    "    lda_model_corpus = lda_model[corpus]\n",
    "    art_topic_keywords = pd.DataFrame(columns = ['article','best_topic','likelyhood','keywords'])\n",
    "    \n",
    "    # Loop through articles to get each article's main topic\n",
    "    for i, article in enumerate(lda_model[corpus]):\n",
    "        for article in enumerate(article): \n",
    "            if article[0] == 0:\n",
    "                #print(article)\n",
    "                wp = optimal_model.show_topic(article[1][0][0])\n",
    "                topic_keywords = ', '.join([word for word, prop in wp])\n",
    "                \n",
    "                best_topic = int(article[1][0][0])\n",
    "                max_likelyhood = round(article[1][0][1],4)\n",
    "                \n",
    "                d = {'article': i, 'best_topic': best_topic,'likelyhood': max_likelyhood,'keywords': topic_keywords}\n",
    "                art_topic_keywords = art_topic_keywords.append(d, ignore_index=True)\n",
    "\n",
    "    return art_topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_gensim_opt_model = optimal_model\n",
    "gensim_topic_df = get_best_gensim_topics(lda_gensim_opt_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>best_topic</th>\n",
       "      <th>likelyhood</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.7154</td>\n",
       "      <td>say, trade, would, export, official, country, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>say, company, share, offer, would, stock, buy,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>say, soybean, acre, program, regular, acreage,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>say, rate, growth, economy, would, economic, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>dollar, market, bank, dealer, yen, money, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>10783</td>\n",
       "      <td>4</td>\n",
       "      <td>0.9260</td>\n",
       "      <td>dollar, market, bank, dealer, yen, money, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>10784</td>\n",
       "      <td>20</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>ct, record, dividend, stock, split, share, may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>10785</td>\n",
       "      <td>20</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>ct, record, dividend, stock, split, share, may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>10786</td>\n",
       "      <td>18</td>\n",
       "      <td>0.8886</td>\n",
       "      <td>loss, ct, net, profit, dlrs, year, note, inclu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>10787</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>say, rate, growth, economy, would, economic, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article best_topic  likelyhood  \\\n",
       "0           0          6      0.7154   \n",
       "1           1          3      0.1170   \n",
       "2           2          2      0.0152   \n",
       "3           3          0      0.1791   \n",
       "4           4          4      0.0337   \n",
       "...       ...        ...         ...   \n",
       "10783   10783          4      0.9260   \n",
       "10784   10784         20      0.7596   \n",
       "10785   10785         20      0.7596   \n",
       "10786   10786         18      0.8886   \n",
       "10787   10787          0      0.0385   \n",
       "\n",
       "                                                keywords  \n",
       "0      say, trade, would, export, official, country, ...  \n",
       "1      say, company, share, offer, would, stock, buy,...  \n",
       "2      say, soybean, acre, program, regular, acreage,...  \n",
       "3      say, rate, growth, economy, would, economic, d...  \n",
       "4      dollar, market, bank, dealer, yen, money, say,...  \n",
       "...                                                  ...  \n",
       "10783  dollar, market, bank, dealer, yen, money, say,...  \n",
       "10784  ct, record, dividend, stock, split, share, may...  \n",
       "10785  ct, record, dividend, stock, split, share, may...  \n",
       "10786  loss, ct, net, profit, dlrs, year, note, inclu...  \n",
       "10787  say, rate, growth, economy, would, economic, d...  \n",
       "\n",
       "[10788 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Mallet Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best topic for each article along with associate keywords and likelyhood\n",
    "def get_best_mallet_topics(lda_model, corpus):\n",
    "    lda_model_corpus = lda_model[corpus]\n",
    "    art_topic_keywords = pd.DataFrame(columns = ['article','best_topic','likelyhood','keywords'])\n",
    "    \n",
    "    for i in range(len(lda_model_corpus)):\n",
    "        # Get article topic model distribution\n",
    "        article = lda_model_corpus[i]\n",
    "\n",
    "        # Find the max likelyhood index and select the corresponding topic\n",
    "        likelyhood = [likelyhood for topic,likelyhood in article]\n",
    "        max_likelyhood = max(likelyhood)\n",
    "        best_topic = likelyhood.index(max_likelyhood)\n",
    "        \n",
    "        # Get keywords for best topcis\n",
    "        wp = lda_model.show_topic(best_topic)\n",
    "        topic_keywords = ', '.join([word for word, prop in wp])\n",
    "        \n",
    "        # Assemble and append information to dataframe\n",
    "        d = {'article': i, 'best_topic': best_topic,'likelyhood': max_likelyhood,'keywords': topic_keywords}\n",
    "        art_topic_keywords = art_topic_keywords.append(d, ignore_index=True)\n",
    "        \n",
    "    return art_topic_keywords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_topic_df = get_best_mallet_topics(ldamallet, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>best_topic</th>\n",
       "      <th>likelyhood</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.258945</td>\n",
       "      <td>official, japanese, government, source, indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.121967</td>\n",
       "      <td>growth, government, economic, year, economy, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0.231944</td>\n",
       "      <td>oil, price, production, crude, barrel, increas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>0.199495</td>\n",
       "      <td>year, export, month, end, import, record, tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0.198211</td>\n",
       "      <td>oil, price, production, crude, barrel, increas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>10783</td>\n",
       "      <td>3</td>\n",
       "      <td>0.128704</td>\n",
       "      <td>dollar, bank, market, exchange, currency, fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>10784</td>\n",
       "      <td>16</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>10785</td>\n",
       "      <td>16</td>\n",
       "      <td>0.084906</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>10786</td>\n",
       "      <td>16</td>\n",
       "      <td>0.263889</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>10787</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>dlrs, year, include, gold, total, reserve, est...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      article best_topic  likelyhood  \\\n",
       "0           0         10    0.258945   \n",
       "1           1         12    0.121967   \n",
       "2           2         17    0.231944   \n",
       "3           3         15    0.199495   \n",
       "4           4         17    0.198211   \n",
       "...       ...        ...         ...   \n",
       "10783   10783          3    0.128704   \n",
       "10784   10784         16    0.084906   \n",
       "10785   10785         16    0.084906   \n",
       "10786   10786         16    0.263889   \n",
       "10787   10787          0    0.050000   \n",
       "\n",
       "                                                keywords  \n",
       "0      official, japanese, government, source, indust...  \n",
       "1      growth, government, economic, year, economy, d...  \n",
       "2      oil, price, production, crude, barrel, increas...  \n",
       "3      year, export, month, end, import, record, tota...  \n",
       "4      oil, price, production, crude, barrel, increas...  \n",
       "...                                                  ...  \n",
       "10783  dollar, bank, market, exchange, currency, fore...  \n",
       "10784  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10785  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10786  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10787  dlrs, year, include, gold, total, reserve, est...  \n",
       "\n",
       "[10788 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mallet_topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss overlap between columns. Maybe read an example article to validate. Talk about verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary dataframe with keywords from each model vs known categories\n",
    "summary_df = pd.concat([df['categories'], gensim_topic_df['keywords'], mallet_topic_df['keywords']], axis=1, sort=False)\n",
    "summary_df.columns = ['reuters_categories','gensim_keywords','mallet_keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reuters_categories</th>\n",
       "      <th>gensim_keywords</th>\n",
       "      <th>mallet_keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[trade]</td>\n",
       "      <td>say, trade, would, export, official, country, ...</td>\n",
       "      <td>official, japanese, government, source, indust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[grain]</td>\n",
       "      <td>say, company, share, offer, would, stock, buy,...</td>\n",
       "      <td>growth, government, economic, year, economy, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[crude, nat-gas]</td>\n",
       "      <td>say, soybean, acre, program, regular, acreage,...</td>\n",
       "      <td>oil, price, production, crude, barrel, increas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[corn, grain, rice, rubber, sugar, tin, trade]</td>\n",
       "      <td>say, rate, growth, economy, would, economic, d...</td>\n",
       "      <td>year, export, month, end, import, record, tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[palm-oil, veg-oil]</td>\n",
       "      <td>dollar, market, bank, dealer, yen, money, say,...</td>\n",
       "      <td>oil, price, production, crude, barrel, increas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>[interest, money-fx]</td>\n",
       "      <td>dollar, market, bank, dealer, yen, money, say,...</td>\n",
       "      <td>dollar, bank, market, exchange, currency, fore...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>ct, record, dividend, stock, split, share, may...</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>ct, record, dividend, stock, split, share, may...</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>loss, ct, net, profit, dlrs, year, note, inclu...</td>\n",
       "      <td>ct, loss, net, profit, note, include, gain, dl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>[earn]</td>\n",
       "      <td>say, rate, growth, economy, would, economic, d...</td>\n",
       "      <td>dlrs, year, include, gold, total, reserve, est...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   reuters_categories  \\\n",
       "0                                             [trade]   \n",
       "1                                             [grain]   \n",
       "2                                    [crude, nat-gas]   \n",
       "3      [corn, grain, rice, rubber, sugar, tin, trade]   \n",
       "4                                 [palm-oil, veg-oil]   \n",
       "...                                               ...   \n",
       "10783                            [interest, money-fx]   \n",
       "10784                                          [earn]   \n",
       "10785                                          [earn]   \n",
       "10786                                          [earn]   \n",
       "10787                                          [earn]   \n",
       "\n",
       "                                         gensim_keywords  \\\n",
       "0      say, trade, would, export, official, country, ...   \n",
       "1      say, company, share, offer, would, stock, buy,...   \n",
       "2      say, soybean, acre, program, regular, acreage,...   \n",
       "3      say, rate, growth, economy, would, economic, d...   \n",
       "4      dollar, market, bank, dealer, yen, money, say,...   \n",
       "...                                                  ...   \n",
       "10783  dollar, market, bank, dealer, yen, money, say,...   \n",
       "10784  ct, record, dividend, stock, split, share, may...   \n",
       "10785  ct, record, dividend, stock, split, share, may...   \n",
       "10786  loss, ct, net, profit, dlrs, year, note, inclu...   \n",
       "10787  say, rate, growth, economy, would, economic, d...   \n",
       "\n",
       "                                         mallet_keywords  \n",
       "0      official, japanese, government, source, indust...  \n",
       "1      growth, government, economic, year, economy, d...  \n",
       "2      oil, price, production, crude, barrel, increas...  \n",
       "3      year, export, month, end, import, record, tota...  \n",
       "4      oil, price, production, crude, barrel, increas...  \n",
       "...                                                  ...  \n",
       "10783  dollar, bank, market, exchange, currency, fore...  \n",
       "10784  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10785  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10786  ct, loss, net, profit, note, include, gain, dl...  \n",
       "10787  dlrs, year, include, gold, total, reserve, est...  \n",
       "\n",
       "[10788 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print chosen dataframe to csv file\n",
    "mallet_topic_df.to_csv('mallet_df.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Analysis\n",
    "\n",
    "- Expecting a disconnected graph with num_topics central nodes with edges to articles. \n",
    "- Get the top 5 largest subgraphs which represent the most common topics (degree centrality)\n",
    "- From the most popular topics, suggest next 5 articles to read by looking at edge weight (likelyhood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataframe from csv file and build a graph from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import networkx.algorithms.bipartite as bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'art_topic_keywords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-8097af01ec47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mG_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas_edgelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mart_topic_keywords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'article'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'best_topic'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mG_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Full bipartite graph'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'art_topic_keywords' is not defined"
     ]
    }
   ],
   "source": [
    "G_full = nx.from_pandas_edgelist(art_topic_keywords,'article','best_topic')\n",
    "G_full.graph['name'] = 'Full bipartite graph'\n",
    "print(nx.info(G_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Graph is bipartite: {} \\nGraph is connected: {} \\nNumber of connected components {}'\n",
    "print(s.format(nx.is_bipartite(G_full), nx.is_connected(G_full), nx.number_connected_components(G_full)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify connected components with greater than five nodes\n",
    "sorted([len(c) for c in nx.connected_components(G_full) if len(c) > 5], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract subgraphs\n",
    "def get_subgraphs(graph):\n",
    "    \n",
    "    subgraphs = [(graph.subgraph(c),len(c)) for c in nx.connected_components(graph) if len(c) > 5] # networkx 2.4\n",
    "    return sorted(subgraphs, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "# Create connected subgraphs and confirm\n",
    "subgraphs = get_subgraphs(G_full)\n",
    "print(*subgraphs[:5], '\\n', sep='\\n')\n",
    "\n",
    "# Isolate the largest subgraph\n",
    "largest_subg = subgraphs[0][0]\n",
    "largest_subg.graph['name'] = 'Main bipartite subgraph'\n",
    "print(nx.info(largest_subg), \"\\n\")\n",
    "\n",
    "# Verify largest subgraph is bipartite\n",
    "G = largest_subg\n",
    "print('Graph is bipartite: {} \\nGraph is connected: {} \\n'.format(nx.is_bipartite(G), nx.is_connected(G)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source node labels\n",
    "graph = G\n",
    "articles, topics = nx.bipartite.sets(graph)\n",
    "\n",
    "# Plot graph by node type\n",
    "# Apply plot settings\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.rcParams['figure.figsize'] = (24,12)\n",
    "plt.axis('off')\n",
    "pos = nx.spring_layout(graph)\n",
    "nx.draw_networkx_nodes(graph, pos, nodelist=articles, node_color='red', alpha = 0.4)\n",
    "nx.draw_networkx_nodes(graph, pos, nodelist=topics, node_color='blue', alpha = 0.4, node_size = 1000)\n",
    "nx.draw_networkx_edges(graph, pos, alpha = 0.4)\n",
    "nx.draw_networkx_labels(graph, pos);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(JO: add remaining references)\n",
    "\n",
    "**Topic Modeling**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Topic_model\n",
    "https://springerplus.springeropen.com/articles/10.1186/s40064-016-3252-8\n",
    "https://mimno.infosci.cornell.edu/papers/2017_fntir_tm_applications.pdf\n",
    "https://cfss.uchicago.edu/notes/topic-modeling/\n",
    "\n",
    "**LDA**\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "https://en.wikipedia.org/wiki/Dirichlet_distribution https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158\n",
    "\n",
    "**Perplexity and Coherence**\n",
    "https://en.wikipedia.org/wiki/Perplexity\n",
    "https://cfss.uchicago.edu/notes/topic-modeling/ (provides examples on optimizing perplexity\n",
    "https://mimno.infosci.cornell.edu/info6150/readings/N10-1012.pdf (automatic topic coherence evaluation)\n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "http://qpleple.com/topic-coherence-to-evaluate-topic-models/ (intrinsic / extrinsic measures and associated math)\n",
    "\n",
    "**Gensim**\n",
    "https://radimrehurek.com/gensim/auto_examples/index.html\n",
    "https://www.thinkinfi.com/2019/08/LDA-Gensim-Python.html\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619\n",
    "\n",
    "**Spacy**\n",
    "https://spacy.io/\n",
    "\n",
    "**MALLET**\n",
    "http://mallet.cs.umass.edu/topics.php\n",
    "https://www.thinkinfi.com/2019/08/LDA-Gensim-Python.html\n",
    "\n",
    "**Text Network Analysis**\n",
    "https://advances.sciencemag.org/content/4/7/eaaq1360\n",
    "https://noduslabs.com/wp-content/uploads/2019/06/InfraNodus-Paranyushkin-WWW19-Conference.pdf\n",
    "https://noduslabs.com/cases/tutorial-lda-text-mining-network-analysis/\n",
    "https://github.com/michal-pikusa/text-network-analysis\n",
    "https://github.com/martingerlach/hSBM_Topicmodel/blob/master/TopSBM-tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
